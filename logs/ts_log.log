2022-02-26T19:49:23,611 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-26T19:49:23,611 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-26T19:49:23,899 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages
Current directory: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist
Temp directory: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4096 M
Python executable: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python
Config file: ./model-server/config.properties
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Initial Models: mnist=mnist.mar
Log dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Metrics dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 8
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Model config: N/A
2022-02-26T19:49:23,899 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages
Current directory: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist
Temp directory: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4096 M
Python executable: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python
Config file: ./model-server/config.properties
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Initial Models: mnist=mnist.mar
Log dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Metrics dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 8
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Model config: N/A
2022-02-26T19:49:23,912 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-26T19:49:23,912 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-26T19:49:23,939 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnist.mar
2022-02-26T19:49:23,939 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnist.mar
2022-02-26T19:49:24,104 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnist
2022-02-26T19:49:24,104 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnist
2022-02-26T19:49:24,104 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnist
2022-02-26T19:49:24,104 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnist
2022-02-26T19:49:24,104 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnist loaded.
2022-02-26T19:49:24,104 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnist loaded.
2022-02-26T19:49:24,104 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnist, count: 8
2022-02-26T19:49:24,104 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnist, count: 8
2022-02-26T19:49:24,121 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T19:49:24,121 [DEBUG] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9004]
2022-02-26T19:49:24,122 [DEBUG] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9003]
2022-02-26T19:49:24,122 [DEBUG] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9002]
2022-02-26T19:49:24,122 [DEBUG] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9007]
2022-02-26T19:49:24,122 [DEBUG] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9006]
2022-02-26T19:49:24,121 [DEBUG] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9005]
2022-02-26T19:49:24,122 [DEBUG] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9002]
2022-02-26T19:49:24,121 [DEBUG] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9005]
2022-02-26T19:49:24,121 [DEBUG] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9004]
2022-02-26T19:49:24,122 [DEBUG] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9006]
2022-02-26T19:49:24,122 [DEBUG] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9003]
2022-02-26T19:49:24,121 [DEBUG] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9001]
2022-02-26T19:49:24,122 [DEBUG] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9007]
2022-02-26T19:49:24,121 [DEBUG] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9001]
2022-02-26T19:49:24,121 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T19:49:24,129 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2022-02-26T19:49:24,129 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2022-02-26T19:49:24,286 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-26T19:49:24,286 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-26T19:49:24,286 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2022-02-26T19:49:24,286 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2022-02-26T19:49:24,287 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-26T19:49:24,287 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-26T19:49:24,288 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2022-02-26T19:49:24,288 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2022-02-26T19:49:24,288 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-26T19:49:24,288 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-26T19:49:24,297 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T19:49:24,297 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T19:49:24,297 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T19:49:24,297 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T19:49:24,298 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T19:49:24,298 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T19:49:24,299 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T19:49:24,299 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T19:49:24,298 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T19:49:24,299 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T19:49:24,299 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T19:49:24,299 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T19:49:24,299 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T19:49:24,299 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T19:49:24,299 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T19:49:24,300 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T19:49:24,300 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T19:49:24,299 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T19:49:24,299 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T19:49:24,300 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T19:49:24,300 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T19:49:24,300 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T19:49:24,301 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T19:49:24,300 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T19:49:24,300 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T19:49:24,301 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T19:49:24,301 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T19:49:24,301 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T19:49:24,301 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T19:49:24,301 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T19:49:24,301 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T19:49:24,301 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T19:49:24,301 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T19:49:24,301 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T19:49:24,301 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T19:49:24,301 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T19:49:24,302 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T19:49:24,302 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T19:49:24,302 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T19:49:24,302 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T19:49:24,305 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T19:49:24,306 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T19:49:24,306 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T19:49:24,306 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T19:49:24,306 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T19:49:24,307 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T19:49:24,307 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T19:49:24,307 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T19:49:24,307 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T19:49:24,308 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T19:49:24,308 [INFO ] W-9005-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-mnist_1.0-stderr
2022-02-26T19:49:24,308 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T19:49:24,308 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T19:49:24,308 [INFO ] W-9007-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-mnist_1.0-stdout
2022-02-26T19:49:24,309 [INFO ] W-9007-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-mnist_1.0-stderr
2022-02-26T19:49:24,308 [INFO ] W-9005-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-mnist_1.0-stdout
2022-02-26T19:49:24,308 [INFO ] W-9005-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-mnist_1.0-stderr
2022-02-26T19:49:24,308 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T19:49:24,309 [INFO ] W-9007-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-mnist_1.0-stderr
2022-02-26T19:49:24,308 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T19:49:24,308 [INFO ] W-9007-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-mnist_1.0-stdout
2022-02-26T19:49:24,308 [INFO ] W-9005-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-mnist_1.0-stdout
2022-02-26T19:49:24,311 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T19:49:24,309 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T19:49:24,311 [WARN ] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-mnist_1.0-stderr
2022-02-26T19:49:24,311 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T19:49:24,311 [WARN ] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-mnist_1.0-stderr
2022-02-26T19:49:24,311 [WARN ] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-mnist_1.0-stderr
2022-02-26T19:49:24,311 [WARN ] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-mnist_1.0-stderr
2022-02-26T19:49:24,311 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T19:49:24,311 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T19:49:24,311 [WARN ] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-mnist_1.0-stdout
2022-02-26T19:49:24,311 [WARN ] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-mnist_1.0-stdout
2022-02-26T19:49:24,311 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T19:49:24,311 [WARN ] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-mnist_1.0-stdout
2022-02-26T19:49:24,311 [WARN ] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-mnist_1.0-stdout
2022-02-26T19:49:24,311 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T19:49:24,311 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T19:49:24,312 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T19:49:24,312 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T19:49:24,311 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T19:49:24,312 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T19:49:24,312 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T19:49:24,312 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T19:49:24,313 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T19:49:24,313 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T19:49:24,313 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T19:49:24,313 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T19:49:24,313 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T19:49:24,313 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T19:49:24,313 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T19:49:24,313 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T19:49:24,314 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T19:49:24,314 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T19:49:24,316 [INFO ] W-9006-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-mnist_1.0-stderr
2022-02-26T19:49:24,316 [INFO ] W-9006-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-mnist_1.0-stderr
2022-02-26T19:49:24,316 [INFO ] W-9006-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-mnist_1.0-stdout
2022-02-26T19:49:24,317 [WARN ] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-mnist_1.0-stderr
2022-02-26T19:49:24,316 [INFO ] W-9006-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-mnist_1.0-stdout
2022-02-26T19:49:24,317 [WARN ] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-mnist_1.0-stderr
2022-02-26T19:49:24,317 [WARN ] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-mnist_1.0-stdout
2022-02-26T19:49:24,317 [WARN ] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-mnist_1.0-stdout
2022-02-26T19:49:24,320 [INFO ] W-9001-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-mnist_1.0-stderr
2022-02-26T19:49:24,320 [INFO ] W-9001-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-mnist_1.0-stderr
2022-02-26T19:49:24,320 [INFO ] W-9001-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-mnist_1.0-stdout
2022-02-26T19:49:24,320 [WARN ] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-mnist_1.0-stderr
2022-02-26T19:49:24,320 [WARN ] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-mnist_1.0-stderr
2022-02-26T19:49:24,320 [INFO ] W-9001-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-mnist_1.0-stdout
2022-02-26T19:49:24,320 [WARN ] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-mnist_1.0-stdout
2022-02-26T19:49:24,320 [WARN ] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-mnist_1.0-stdout
2022-02-26T19:49:24,321 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T19:49:24,321 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T19:49:24,321 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T19:49:24,322 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T19:49:24,322 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T19:49:24,322 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T19:49:24,322 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T19:49:24,322 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T19:49:24,322 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T19:49:24,323 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T19:49:24,321 [ERROR] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T19:49:24,311 [ERROR] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T19:49:24,318 [ERROR] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T19:49:24,311 [ERROR] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T19:49:24,311 [ERROR] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T19:49:24,331 [INFO ] W-9003-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-mnist_1.0-stderr
2022-02-26T19:49:24,321 [ERROR] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T19:49:24,318 [ERROR] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T19:49:24,331 [INFO ] W-9003-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-mnist_1.0-stdout
2022-02-26T19:49:24,311 [ERROR] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T19:49:24,311 [ERROR] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T19:49:24,331 [INFO ] W-9003-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-mnist_1.0-stderr
2022-02-26T19:49:24,331 [INFO ] W-9003-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-mnist_1.0-stdout
2022-02-26T19:49:24,311 [ERROR] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T19:49:24,331 [WARN ] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-mnist_1.0-stderr
2022-02-26T19:49:24,331 [WARN ] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-mnist_1.0-stderr
2022-02-26T19:49:24,332 [WARN ] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-mnist_1.0-stdout
2022-02-26T19:49:24,332 [WARN ] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-mnist_1.0-stdout
2022-02-26T19:49:24,332 [ERROR] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T19:49:24,332 [INFO ] W-9004-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-mnist_1.0-stderr
2022-02-26T19:49:24,332 [INFO ] W-9004-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-mnist_1.0-stderr
2022-02-26T19:49:24,332 [ERROR] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T19:49:24,332 [INFO ] W-9004-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-mnist_1.0-stdout
2022-02-26T19:49:24,333 [WARN ] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-mnist_1.0-stderr
2022-02-26T19:49:24,332 [INFO ] W-9004-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-mnist_1.0-stdout
2022-02-26T19:49:24,333 [WARN ] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-mnist_1.0-stderr
2022-02-26T19:49:24,334 [INFO ] W-9002-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-mnist_1.0-stderr
2022-02-26T19:49:24,334 [WARN ] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-mnist_1.0-stdout
2022-02-26T19:49:24,334 [INFO ] W-9002-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-mnist_1.0-stderr
2022-02-26T19:49:24,334 [WARN ] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-mnist_1.0-stdout
2022-02-26T19:49:24,334 [INFO ] W-9002-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-mnist_1.0-stdout
2022-02-26T19:49:24,334 [WARN ] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-mnist_1.0-stderr
2022-02-26T19:49:24,334 [WARN ] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-mnist_1.0-stderr
2022-02-26T19:49:24,334 [INFO ] W-9002-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-mnist_1.0-stdout
2022-02-26T19:49:24,334 [ERROR] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T19:49:24,334 [WARN ] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-mnist_1.0-stdout
2022-02-26T19:49:24,334 [WARN ] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-mnist_1.0-stdout
2022-02-26T19:49:24,334 [ERROR] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T19:49:24,335 [ERROR] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T19:49:24,335 [ERROR] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T19:49:24,575 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T19:49:24,575 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T19:49:24,628 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645904964
2022-02-26T19:49:24,629 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:256.0616455078125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645904964
2022-02-26T19:49:24,629 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645904964
2022-02-26T19:49:24,629 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645904964
2022-02-26T19:49:24,629 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5061.73046875|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645904964
2022-02-26T19:49:24,630 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8166.4921875|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645904964
2022-02-26T19:49:24,630 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:69.1|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645904964
2022-02-26T19:49:43,951 [INFO ] W-9002-mnist_1.0 ACCESS_LOG - /127.0.0.1:54513 "GET /ping HTTP/1.1" 200 5
2022-02-26T19:49:43,951 [INFO ] W-9002-mnist_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:null
2022-02-26T19:50:24,575 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T19:50:24,575 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T19:50:24,625 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905024
2022-02-26T19:50:24,625 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:256.06311798095703|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905024
2022-02-26T19:50:24,626 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905024
2022-02-26T19:50:24,626 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905024
2022-02-26T19:50:24,626 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5134.39453125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905024
2022-02-26T19:50:24,626 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8096.15625|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905024
2022-02-26T19:50:24,627 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:68.7|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905024
2022-02-26T19:51:24,575 [WARN ] pool-3-thread-2 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T19:51:24,575 [WARN ] pool-3-thread-2 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T19:51:24,641 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905084
2022-02-26T19:51:24,642 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:256.0629539489746|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905084
2022-02-26T19:51:24,643 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905084
2022-02-26T19:51:24,643 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905084
2022-02-26T19:51:24,643 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:5103.25|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905084
2022-02-26T19:51:24,643 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:8127.25|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905084
2022-02-26T19:51:24,643 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:68.9|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905084
2022-02-26T19:52:24,574 [WARN ] pool-3-thread-2 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T19:52:24,574 [WARN ] pool-3-thread-2 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T19:52:24,683 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905144
2022-02-26T19:52:24,684 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:256.055721282959|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905144
2022-02-26T19:52:24,684 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905144
2022-02-26T19:52:24,685 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905144
2022-02-26T19:52:24,685 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:5152.01171875|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905144
2022-02-26T19:52:24,685 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:8066.78125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905144
2022-02-26T19:52:24,685 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:68.6|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905144
2022-02-26T19:53:24,573 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T19:53:24,573 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T19:53:24,721 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905204
2022-02-26T19:53:24,722 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:256.06353759765625|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905204
2022-02-26T19:53:24,722 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905204
2022-02-26T19:53:24,722 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905204
2022-02-26T19:53:24,722 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5050.71875|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905204
2022-02-26T19:53:24,722 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8163.30859375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905204
2022-02-26T19:53:24,723 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:69.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905204
2022-02-26T19:54:24,574 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T19:54:24,574 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T19:54:24,663 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905264
2022-02-26T19:54:24,663 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:256.0908088684082|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905264
2022-02-26T19:54:24,664 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905264
2022-02-26T19:54:24,664 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905264
2022-02-26T19:54:24,664 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5180.59375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905264
2022-02-26T19:54:24,664 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8028.85546875|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905264
2022-02-26T19:54:24,664 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:68.4|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905264
2022-02-26T19:55:24,578 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T19:55:24,578 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T19:55:24,699 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905324
2022-02-26T19:55:24,699 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:256.0883445739746|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905324
2022-02-26T19:55:24,700 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905324
2022-02-26T19:55:24,700 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905324
2022-02-26T19:55:24,700 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5186.98046875|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905324
2022-02-26T19:55:24,700 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8029.64453125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905324
2022-02-26T19:55:24,700 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:68.3|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905324
2022-02-26T19:56:24,569 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T19:56:24,569 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T19:56:24,619 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905384
2022-02-26T19:56:24,619 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:256.0879554748535|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905384
2022-02-26T19:56:24,619 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905384
2022-02-26T19:56:24,619 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905384
2022-02-26T19:56:24,620 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5183.83203125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905384
2022-02-26T19:56:24,620 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8033.90234375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905384
2022-02-26T19:56:24,620 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:68.4|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905384
2022-02-26T19:57:24,566 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T19:57:24,566 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T19:57:24,618 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905444
2022-02-26T19:57:24,618 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:256.08851623535156|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905444
2022-02-26T19:57:24,618 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905444
2022-02-26T19:57:24,618 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905444
2022-02-26T19:57:24,618 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5184.16015625|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905444
2022-02-26T19:57:24,619 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8036.484375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905444
2022-02-26T19:57:24,619 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:68.4|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905444
2022-02-26T19:58:24,577 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T19:58:24,577 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T19:58:24,696 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905504
2022-02-26T19:58:24,697 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:256.0894660949707|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905504
2022-02-26T19:58:24,697 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905504
2022-02-26T19:58:24,697 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905504
2022-02-26T19:58:24,697 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5179.4375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905504
2022-02-26T19:58:24,697 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8020.69921875|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905504
2022-02-26T19:58:24,697 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:68.4|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905504
2022-02-26T19:59:24,570 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T19:59:24,570 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T19:59:24,656 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905564
2022-02-26T19:59:24,656 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:256.08688735961914|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905564
2022-02-26T19:59:24,656 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905564
2022-02-26T19:59:24,657 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905564
2022-02-26T19:59:24,657 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5176.7109375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905564
2022-02-26T19:59:24,657 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8015.703125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905564
2022-02-26T19:59:24,657 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:68.4|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905564
2022-02-26T20:00:24,568 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:00:24,568 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:00:24,664 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905624
2022-02-26T20:00:24,665 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:256.0972366333008|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905624
2022-02-26T20:00:24,665 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905624
2022-02-26T20:00:24,665 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905624
2022-02-26T20:00:24,666 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:4779.02734375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905624
2022-02-26T20:00:24,666 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8472.0546875|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905624
2022-02-26T20:00:24,666 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:70.8|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905624
2022-02-26T20:01:24,568 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:01:24,568 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:01:24,671 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905684
2022-02-26T20:01:24,672 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:256.0530700683594|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905684
2022-02-26T20:01:24,672 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905684
2022-02-26T20:01:24,672 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905684
2022-02-26T20:01:24,672 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5070.14453125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905684
2022-02-26T20:01:24,673 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8150.3125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905684
2022-02-26T20:01:24,673 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:69.1|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905684
2022-02-26T20:02:24,566 [WARN ] pool-3-thread-2 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:02:24,566 [WARN ] pool-3-thread-2 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:02:24,673 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905744
2022-02-26T20:02:24,674 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:256.0554313659668|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905744
2022-02-26T20:02:24,674 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905744
2022-02-26T20:02:24,674 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905744
2022-02-26T20:02:24,674 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:5018.76171875|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905744
2022-02-26T20:02:24,674 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:8177.953125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905744
2022-02-26T20:02:24,675 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:69.4|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905744
2022-02-26T20:02:47,780 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-26T20:02:47,780 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-26T20:02:48,096 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages
Current directory: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist
Temp directory: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4096 M
Python executable: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python
Config file: ./model-server/config.properties
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Initial Models: mnist=mnist.mar
Log dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Metrics dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 8
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Model config: N/A
2022-02-26T20:02:48,096 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages
Current directory: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist
Temp directory: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4096 M
Python executable: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python
Config file: ./model-server/config.properties
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Initial Models: mnist=mnist.mar
Log dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Metrics dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 8
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Model config: N/A
2022-02-26T20:02:48,104 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-26T20:02:48,104 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-26T20:02:48,125 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnist.mar
2022-02-26T20:02:48,125 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnist.mar
2022-02-26T20:02:48,277 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnist
2022-02-26T20:02:48,277 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnist
2022-02-26T20:02:48,277 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnist
2022-02-26T20:02:48,277 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnist
2022-02-26T20:02:48,277 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnist loaded.
2022-02-26T20:02:48,277 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnist loaded.
2022-02-26T20:02:48,278 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnist, count: 8
2022-02-26T20:02:48,278 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnist, count: 8
2022-02-26T20:02:48,289 [DEBUG] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9004]
2022-02-26T20:02:48,290 [DEBUG] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9006]
2022-02-26T20:02:48,290 [DEBUG] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9001]
2022-02-26T20:02:48,289 [DEBUG] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9005]
2022-02-26T20:02:48,289 [DEBUG] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9003]
2022-02-26T20:02:48,289 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:02:48,290 [DEBUG] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9007]
2022-02-26T20:02:48,289 [DEBUG] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9005]
2022-02-26T20:02:48,289 [DEBUG] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9002]
2022-02-26T20:02:48,289 [DEBUG] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9003]
2022-02-26T20:02:48,289 [DEBUG] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9002]
2022-02-26T20:02:48,289 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:02:48,289 [DEBUG] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9004]
2022-02-26T20:02:48,290 [DEBUG] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9006]
2022-02-26T20:02:48,290 [DEBUG] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9007]
2022-02-26T20:02:48,290 [DEBUG] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9001]
2022-02-26T20:02:48,293 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2022-02-26T20:02:48,293 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2022-02-26T20:02:48,444 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:02:48,444 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:02:48,446 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:02:48,446 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:02:48,446 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:02:48,446 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:02:48,446 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:02:48,447 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:02:48,447 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:02:48,447 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:02:48,447 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:02:48,447 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:02:48,447 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:02:48,447 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:02:48,447 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:02:48,448 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:02:48,448 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:02:48,448 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:02:48,448 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:02:48,448 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:02:48,453 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:02:48,454 [INFO ] W-9005-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-mnist_1.0-stderr
2022-02-26T20:02:48,454 [INFO ] W-9006-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-mnist_1.0-stderr
2022-02-26T20:02:48,454 [INFO ] W-9005-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-mnist_1.0-stderr
2022-02-26T20:02:48,454 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:02:48,454 [INFO ] W-9006-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-mnist_1.0-stderr
2022-02-26T20:02:48,454 [INFO ] W-9006-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-mnist_1.0-stdout
2022-02-26T20:02:48,454 [INFO ] W-9005-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-mnist_1.0-stdout
2022-02-26T20:02:48,455 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:02:48,455 [WARN ] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-mnist_1.0-stderr
2022-02-26T20:02:48,455 [WARN ] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-mnist_1.0-stderr
2022-02-26T20:02:48,454 [INFO ] W-9005-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-mnist_1.0-stdout
2022-02-26T20:02:48,454 [INFO ] W-9006-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-mnist_1.0-stdout
2022-02-26T20:02:48,455 [WARN ] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-mnist_1.0-stderr
2022-02-26T20:02:48,455 [WARN ] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-mnist_1.0-stderr
2022-02-26T20:02:48,456 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:02:48,456 [WARN ] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-mnist_1.0-stdout
2022-02-26T20:02:48,456 [WARN ] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-mnist_1.0-stdout
2022-02-26T20:02:48,456 [WARN ] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-mnist_1.0-stdout
2022-02-26T20:02:48,456 [WARN ] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-mnist_1.0-stdout
2022-02-26T20:02:48,456 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:02:48,457 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:02:48,457 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:02:48,457 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:02:48,457 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:02:48,458 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:02:48,459 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:02:48,460 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:02:48,460 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:02:48,460 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:02:48,461 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:02:48,461 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:02:48,461 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:02:48,461 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:02:48,462 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:02:48,462 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:02:48,462 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:02:48,463 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:02:48,463 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:02:48,463 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:02:48,464 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:02:48,464 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:02:48,464 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:02:48,464 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:02:48,464 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:02:48,464 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:02:48,464 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:02:48,465 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:02:48,465 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:02:48,465 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:02:48,465 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:02:48,466 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:02:48,466 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:02:48,466 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:02:48,466 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:02:48,466 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:02:48,467 [INFO ] W-9003-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-mnist_1.0-stderr
2022-02-26T20:02:48,467 [INFO ] W-9003-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-mnist_1.0-stderr
2022-02-26T20:02:48,467 [INFO ] W-9003-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-mnist_1.0-stdout
2022-02-26T20:02:48,468 [WARN ] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-mnist_1.0-stderr
2022-02-26T20:02:48,467 [INFO ] W-9003-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-mnist_1.0-stdout
2022-02-26T20:02:48,468 [WARN ] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-mnist_1.0-stderr
2022-02-26T20:02:48,469 [WARN ] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-mnist_1.0-stdout
2022-02-26T20:02:48,469 [WARN ] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-mnist_1.0-stdout
2022-02-26T20:02:48,472 [INFO ] W-9001-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-mnist_1.0-stderr
2022-02-26T20:02:48,472 [INFO ] W-9001-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-mnist_1.0-stderr
2022-02-26T20:02:48,472 [INFO ] W-9001-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-mnist_1.0-stdout
2022-02-26T20:02:48,473 [WARN ] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-mnist_1.0-stderr
2022-02-26T20:02:48,472 [INFO ] W-9001-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-mnist_1.0-stdout
2022-02-26T20:02:48,473 [WARN ] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-mnist_1.0-stderr
2022-02-26T20:02:48,474 [WARN ] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-mnist_1.0-stdout
2022-02-26T20:02:48,474 [WARN ] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-mnist_1.0-stdout
2022-02-26T20:02:48,469 [ERROR] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:02:48,457 [ERROR] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:02:48,473 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:02:48,457 [ERROR] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:02:48,474 [ERROR] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:02:48,469 [ERROR] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:02:48,457 [ERROR] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:02:48,474 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:02:48,474 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:02:48,474 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:02:48,474 [ERROR] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:02:48,457 [ERROR] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:02:48,475 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:02:48,475 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:02:48,475 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:02:48,475 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:02:48,475 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:02:48,475 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:02:48,475 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:02:48,475 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:02:48,475 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:02:48,475 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:02:48,476 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:02:48,476 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:02:48,476 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:02:48,476 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:02:48,476 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:02:48,476 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:02:48,481 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:02:48,482 [INFO ] W-9004-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-mnist_1.0-stderr
2022-02-26T20:02:48,481 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:02:48,482 [INFO ] W-9004-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-mnist_1.0-stderr
2022-02-26T20:02:48,482 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:02:48,482 [INFO ] W-9004-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-mnist_1.0-stdout
2022-02-26T20:02:48,483 [WARN ] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-mnist_1.0-stderr
2022-02-26T20:02:48,483 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:02:48,482 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:02:48,483 [WARN ] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-mnist_1.0-stderr
2022-02-26T20:02:48,483 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:02:48,482 [INFO ] W-9004-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-mnist_1.0-stdout
2022-02-26T20:02:48,483 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:02:48,483 [WARN ] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-mnist_1.0-stdout
2022-02-26T20:02:48,483 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:02:48,483 [WARN ] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-mnist_1.0-stdout
2022-02-26T20:02:48,483 [ERROR] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:02:48,483 [ERROR] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:02:48,483 [ERROR] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:02:48,483 [ERROR] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:02:48,485 [INFO ] W-9002-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-mnist_1.0-stderr
2022-02-26T20:02:48,485 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-26T20:02:48,485 [INFO ] W-9002-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-mnist_1.0-stderr
2022-02-26T20:02:48,485 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-26T20:02:48,485 [INFO ] W-9002-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-mnist_1.0-stdout
2022-02-26T20:02:48,486 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2022-02-26T20:02:48,486 [WARN ] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-mnist_1.0-stderr
2022-02-26T20:02:48,486 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2022-02-26T20:02:48,485 [INFO ] W-9002-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-mnist_1.0-stdout
2022-02-26T20:02:48,486 [WARN ] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-mnist_1.0-stderr
2022-02-26T20:02:48,486 [WARN ] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-mnist_1.0-stdout
2022-02-26T20:02:48,486 [WARN ] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-mnist_1.0-stdout
2022-02-26T20:02:48,486 [ERROR] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:02:48,486 [ERROR] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:02:48,487 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-26T20:02:48,487 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-26T20:02:48,487 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2022-02-26T20:02:48,487 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2022-02-26T20:02:48,488 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-26T20:02:48,488 [INFO ] W-9007-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-mnist_1.0-stderr
2022-02-26T20:02:48,488 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-26T20:02:48,488 [INFO ] W-9007-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-mnist_1.0-stderr
2022-02-26T20:02:48,488 [INFO ] W-9007-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-mnist_1.0-stdout
2022-02-26T20:02:48,489 [WARN ] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-mnist_1.0-stderr
2022-02-26T20:02:48,488 [INFO ] W-9007-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-mnist_1.0-stdout
2022-02-26T20:02:48,489 [WARN ] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-mnist_1.0-stderr
2022-02-26T20:02:48,489 [WARN ] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-mnist_1.0-stdout
2022-02-26T20:02:48,489 [WARN ] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-mnist_1.0-stdout
2022-02-26T20:02:48,489 [ERROR] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:02:48,489 [ERROR] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:02:48,719 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:02:48,719 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:02:48,771 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905768
2022-02-26T20:02:48,771 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:256.0553741455078|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905768
2022-02-26T20:02:48,771 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905768
2022-02-26T20:02:48,772 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905768
2022-02-26T20:02:48,772 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5134.68359375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905768
2022-02-26T20:02:48,773 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8060.39453125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905768
2022-02-26T20:02:48,773 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:68.7|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905768
2022-02-26T20:03:05,316 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-26T20:03:05,316 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-26T20:03:05,434 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages
Current directory: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist
Temp directory: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4096 M
Python executable: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python
Config file: ./model-server/config.properties
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Initial Models: mnist=mnist.mar
Log dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Metrics dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 8
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Model config: N/A
2022-02-26T20:03:05,434 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages
Current directory: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist
Temp directory: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4096 M
Python executable: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python
Config file: ./model-server/config.properties
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Initial Models: mnist=mnist.mar
Log dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Metrics dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 8
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Model config: N/A
2022-02-26T20:03:05,439 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-26T20:03:05,439 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-26T20:03:05,456 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnist.mar
2022-02-26T20:03:05,456 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnist.mar
2022-02-26T20:03:05,604 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnist
2022-02-26T20:03:05,604 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnist
2022-02-26T20:03:05,605 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnist
2022-02-26T20:03:05,605 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnist
2022-02-26T20:03:05,605 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnist loaded.
2022-02-26T20:03:05,605 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnist loaded.
2022-02-26T20:03:05,605 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnist, count: 8
2022-02-26T20:03:05,605 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnist, count: 8
2022-02-26T20:03:05,615 [DEBUG] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9003]
2022-02-26T20:03:05,616 [DEBUG] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9004]
2022-02-26T20:03:05,615 [DEBUG] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9006]
2022-02-26T20:03:05,616 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:03:05,615 [DEBUG] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9005]
2022-02-26T20:03:05,615 [DEBUG] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9003]
2022-02-26T20:03:05,615 [DEBUG] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9001]
2022-02-26T20:03:05,616 [DEBUG] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9002]
2022-02-26T20:03:05,616 [DEBUG] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9004]
2022-02-26T20:03:05,615 [DEBUG] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9005]
2022-02-26T20:03:05,616 [DEBUG] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9007]
2022-02-26T20:03:05,616 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:03:05,615 [DEBUG] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9001]
2022-02-26T20:03:05,616 [DEBUG] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9007]
2022-02-26T20:03:05,615 [DEBUG] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9006]
2022-02-26T20:03:05,616 [DEBUG] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9002]
2022-02-26T20:03:05,618 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2022-02-26T20:03:05,618 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2022-02-26T20:03:05,749 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:03:05,759 [INFO ] W-9003-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-mnist_1.0-stdout
2022-02-26T20:03:05,759 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:03:05,760 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:03:05,759 [INFO ] W-9003-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-mnist_1.0-stdout
2022-02-26T20:03:05,750 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:03:05,761 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:03:05,759 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:03:05,762 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:03:05,762 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:03:05,762 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:03:05,762 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:03:05,762 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:03:05,762 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:03:05,763 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:03:05,763 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:03:05,763 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:03:05,763 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:03:05,763 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:03:05,763 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:03:05,763 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:03:05,763 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:03:05,763 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:03:05,763 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:03:05,763 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:03:05,763 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:03:05,763 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:03:05,763 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:03:05,763 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:03:05,764 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:03:05,764 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:03:05,764 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:03:05,764 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:03:05,764 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:03:05,764 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:03:05,764 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:03:05,764 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:03:05,764 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:03:05,764 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:03:05,764 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:03:05,764 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:03:05,764 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:03:05,764 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:03:05,764 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:03:05,765 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:03:05,765 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:03:05,765 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:03:05,765 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:03:05,766 [INFO ] W-9003-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-mnist_1.0-stderr
2022-02-26T20:03:05,766 [WARN ] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-mnist_1.0-stderr
2022-02-26T20:03:05,766 [INFO ] W-9003-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-mnist_1.0-stderr
2022-02-26T20:03:05,766 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:03:05,766 [WARN ] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-mnist_1.0-stderr
2022-02-26T20:03:05,766 [WARN ] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-mnist_1.0-stdout
2022-02-26T20:03:05,766 [WARN ] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-mnist_1.0-stdout
2022-02-26T20:03:05,766 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:03:05,766 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:03:05,766 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:03:05,767 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:03:05,767 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:03:05,767 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:03:05,767 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:03:05,767 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:03:05,768 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:03:05,768 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:03:05,768 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:03:05,769 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:03:05,769 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:03:05,774 [INFO ] W-9007-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-mnist_1.0-stderr
2022-02-26T20:03:05,774 [INFO ] W-9007-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-mnist_1.0-stderr
2022-02-26T20:03:05,774 [INFO ] W-9007-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-mnist_1.0-stdout
2022-02-26T20:03:05,775 [WARN ] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-mnist_1.0-stderr
2022-02-26T20:03:05,775 [WARN ] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-mnist_1.0-stderr
2022-02-26T20:03:05,774 [INFO ] W-9007-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-mnist_1.0-stdout
2022-02-26T20:03:05,775 [WARN ] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-mnist_1.0-stdout
2022-02-26T20:03:05,775 [WARN ] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-mnist_1.0-stdout
2022-02-26T20:03:05,777 [INFO ] W-9004-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-mnist_1.0-stderr
2022-02-26T20:03:05,777 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:03:05,777 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:03:05,777 [INFO ] W-9004-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-mnist_1.0-stderr
2022-02-26T20:03:05,779 [WARN ] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-mnist_1.0-stderr
2022-02-26T20:03:05,779 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:03:05,777 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:03:05,777 [INFO ] W-9004-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-mnist_1.0-stdout
2022-02-26T20:03:05,779 [WARN ] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-mnist_1.0-stderr
2022-02-26T20:03:05,779 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:03:05,780 [WARN ] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-mnist_1.0-stdout
2022-02-26T20:03:05,780 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:03:05,780 [WARN ] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-mnist_1.0-stdout
2022-02-26T20:03:05,780 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:03:05,777 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:03:05,777 [INFO ] W-9004-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-mnist_1.0-stdout
2022-02-26T20:03:05,784 [INFO ] W-9006-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-mnist_1.0-stderr
2022-02-26T20:03:05,784 [INFO ] W-9006-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-mnist_1.0-stderr
2022-02-26T20:03:05,784 [INFO ] W-9006-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-mnist_1.0-stdout
2022-02-26T20:03:05,782 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:03:05,785 [WARN ] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-mnist_1.0-stderr
2022-02-26T20:03:05,780 [ERROR] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:03:05,766 [ERROR] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:03:05,776 [ERROR] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:03:05,780 [ERROR] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:03:05,785 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:03:05,784 [INFO ] W-9006-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-mnist_1.0-stdout
2022-02-26T20:03:05,785 [WARN ] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-mnist_1.0-stderr
2022-02-26T20:03:05,786 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:03:05,766 [ERROR] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:03:05,780 [ERROR] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:03:05,786 [WARN ] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-mnist_1.0-stdout
2022-02-26T20:03:05,786 [WARN ] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-mnist_1.0-stdout
2022-02-26T20:03:05,786 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:03:05,780 [ERROR] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:03:05,776 [ERROR] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:03:05,786 [ERROR] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:03:05,786 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:03:05,786 [ERROR] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:03:05,786 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:03:05,787 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:03:05,787 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:03:05,787 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:03:05,787 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:03:05,789 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:03:05,789 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:03:05,790 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:03:05,790 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:03:05,790 [INFO ] W-9005-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-mnist_1.0-stderr
2022-02-26T20:03:05,791 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:03:05,790 [INFO ] W-9005-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-mnist_1.0-stderr
2022-02-26T20:03:05,790 [INFO ] W-9005-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-mnist_1.0-stdout
2022-02-26T20:03:05,791 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:03:05,791 [WARN ] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-mnist_1.0-stderr
2022-02-26T20:03:05,790 [INFO ] W-9005-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-mnist_1.0-stdout
2022-02-26T20:03:05,791 [WARN ] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-mnist_1.0-stderr
2022-02-26T20:03:05,791 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:03:05,792 [WARN ] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-mnist_1.0-stdout
2022-02-26T20:03:05,792 [WARN ] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-mnist_1.0-stdout
2022-02-26T20:03:05,792 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:03:05,792 [ERROR] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:03:05,792 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:03:05,792 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:03:05,792 [ERROR] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:03:05,795 [INFO ] W-9002-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-mnist_1.0-stderr
2022-02-26T20:03:05,795 [INFO ] W-9002-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-mnist_1.0-stderr
2022-02-26T20:03:05,795 [INFO ] W-9002-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-mnist_1.0-stdout
2022-02-26T20:03:05,795 [INFO ] W-9002-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-mnist_1.0-stdout
2022-02-26T20:03:05,796 [WARN ] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-mnist_1.0-stderr
2022-02-26T20:03:05,796 [WARN ] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-mnist_1.0-stderr
2022-02-26T20:03:05,796 [WARN ] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-mnist_1.0-stdout
2022-02-26T20:03:05,796 [WARN ] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-mnist_1.0-stdout
2022-02-26T20:03:05,796 [ERROR] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:03:05,796 [ERROR] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:03:05,799 [INFO ] W-9001-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-mnist_1.0-stderr
2022-02-26T20:03:05,799 [INFO ] W-9001-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-mnist_1.0-stderr
2022-02-26T20:03:05,799 [INFO ] W-9001-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-mnist_1.0-stdout
2022-02-26T20:03:05,800 [WARN ] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-mnist_1.0-stderr
2022-02-26T20:03:05,799 [INFO ] W-9001-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-mnist_1.0-stdout
2022-02-26T20:03:05,800 [WARN ] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-mnist_1.0-stderr
2022-02-26T20:03:05,800 [WARN ] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-mnist_1.0-stdout
2022-02-26T20:03:05,800 [WARN ] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-mnist_1.0-stdout
2022-02-26T20:03:05,800 [ERROR] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:03:05,800 [ERROR] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:03:05,815 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-26T20:03:05,815 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-26T20:03:05,815 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2022-02-26T20:03:05,815 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2022-02-26T20:03:05,816 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-26T20:03:05,816 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-26T20:03:05,816 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2022-02-26T20:03:05,816 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2022-02-26T20:03:05,817 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-26T20:03:05,817 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-26T20:03:05,982 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:03:05,982 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:03:06,032 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905786
2022-02-26T20:03:06,033 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:256.0547676086426|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905786
2022-02-26T20:03:06,033 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905786
2022-02-26T20:03:06,033 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905786
2022-02-26T20:03:06,033 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5154.2734375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905786
2022-02-26T20:03:06,033 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8042.00390625|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905786
2022-02-26T20:03:06,034 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:68.5|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645905786
2022-02-26T20:10:24,952 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-26T20:10:24,952 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-26T20:10:25,245 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages
Current directory: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist
Temp directory: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4096 M
Python executable: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python
Config file: ./model-server/config.properties
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Initial Models: mnist=mnist.mar
Log dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Metrics dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 8
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Model config: N/A
2022-02-26T20:10:25,245 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages
Current directory: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist
Temp directory: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4096 M
Python executable: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python
Config file: ./model-server/config.properties
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Initial Models: mnist=mnist.mar
Log dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Metrics dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 8
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Model config: N/A
2022-02-26T20:10:25,257 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-26T20:10:25,257 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-26T20:10:25,283 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnist.mar
2022-02-26T20:10:25,283 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnist.mar
2022-02-26T20:10:25,446 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnist
2022-02-26T20:10:25,446 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnist
2022-02-26T20:10:25,446 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnist
2022-02-26T20:10:25,446 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnist
2022-02-26T20:10:25,446 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnist loaded.
2022-02-26T20:10:25,446 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnist loaded.
2022-02-26T20:10:25,446 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnist, count: 8
2022-02-26T20:10:25,446 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnist, count: 8
2022-02-26T20:10:25,462 [DEBUG] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9004]
2022-02-26T20:10:25,462 [DEBUG] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9007]
2022-02-26T20:10:25,462 [DEBUG] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9003]
2022-02-26T20:10:25,462 [DEBUG] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9006]
2022-02-26T20:10:25,462 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:10:25,462 [DEBUG] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9005]
2022-02-26T20:10:25,462 [DEBUG] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9002]
2022-02-26T20:10:25,462 [DEBUG] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9003]
2022-02-26T20:10:25,462 [DEBUG] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9001]
2022-02-26T20:10:25,462 [DEBUG] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9004]
2022-02-26T20:10:25,462 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:10:25,462 [DEBUG] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9001]
2022-02-26T20:10:25,462 [DEBUG] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9006]
2022-02-26T20:10:25,462 [DEBUG] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9007]
2022-02-26T20:10:25,462 [DEBUG] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9005]
2022-02-26T20:10:25,462 [DEBUG] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9002]
2022-02-26T20:10:25,466 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2022-02-26T20:10:25,466 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2022-02-26T20:10:25,604 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:10:25,606 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:10:25,607 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:10:25,607 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:10:25,608 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:10:25,608 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:10:25,608 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:10:25,609 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:10:25,609 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:10:25,609 [WARN ] W-9006-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:10:25,630 [INFO ] W-9006-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-mnist_1.0-stderr
2022-02-26T20:10:25,630 [INFO ] W-9006-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-mnist_1.0-stderr
2022-02-26T20:10:25,641 [WARN ] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-mnist_1.0-stderr
2022-02-26T20:10:25,641 [WARN ] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-mnist_1.0-stderr
2022-02-26T20:10:25,641 [WARN ] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-mnist_1.0-stdout
2022-02-26T20:10:25,641 [WARN ] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-mnist_1.0-stdout
2022-02-26T20:10:25,641 [INFO ] W-9006-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-mnist_1.0-stdout
2022-02-26T20:10:25,641 [INFO ] W-9006-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-mnist_1.0-stdout
2022-02-26T20:10:25,641 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-26T20:10:25,641 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-26T20:10:25,645 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2022-02-26T20:10:25,645 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2022-02-26T20:10:25,646 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-26T20:10:25,646 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-26T20:10:25,647 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2022-02-26T20:10:25,647 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2022-02-26T20:10:25,651 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-26T20:10:25,651 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-26T20:10:25,641 [ERROR] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:10:25,641 [ERROR] W-9006-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:10:25,656 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:10:25,657 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:10:25,657 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:10:25,658 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:10:25,658 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:10:25,658 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:10:25,658 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:10:25,659 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:10:25,659 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:10:25,659 [WARN ] W-9007-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:10:25,671 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:10:25,671 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:10:25,672 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:10:25,672 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:10:25,672 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:10:25,672 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:10:25,672 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:10:25,672 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:10:25,672 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:10:25,672 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:10:25,672 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:10:25,672 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:10:25,673 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:10:25,673 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:10:25,673 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:10:25,673 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:10:25,673 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:10:25,673 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:10:25,673 [WARN ] W-9002-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:10:25,673 [WARN ] W-9003-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:10:25,682 [INFO ] W-9003-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-mnist_1.0-stderr
2022-02-26T20:10:25,682 [INFO ] W-9003-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-mnist_1.0-stderr
2022-02-26T20:10:25,682 [INFO ] W-9003-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-mnist_1.0-stdout
2022-02-26T20:10:25,683 [INFO ] W-9002-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-mnist_1.0-stderr
2022-02-26T20:10:25,682 [INFO ] W-9003-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-mnist_1.0-stdout
2022-02-26T20:10:25,683 [INFO ] W-9002-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-mnist_1.0-stdout
2022-02-26T20:10:25,683 [INFO ] W-9002-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-mnist_1.0-stderr
2022-02-26T20:10:25,684 [WARN ] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-mnist_1.0-stderr
2022-02-26T20:10:25,684 [WARN ] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-mnist_1.0-stderr
2022-02-26T20:10:25,684 [WARN ] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-mnist_1.0-stderr
2022-02-26T20:10:25,684 [WARN ] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-mnist_1.0-stderr
2022-02-26T20:10:25,683 [INFO ] W-9002-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-mnist_1.0-stdout
2022-02-26T20:10:25,684 [WARN ] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-mnist_1.0-stdout
2022-02-26T20:10:25,685 [WARN ] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-mnist_1.0-stdout
2022-02-26T20:10:25,684 [WARN ] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-mnist_1.0-stdout
2022-02-26T20:10:25,685 [WARN ] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-mnist_1.0-stdout
2022-02-26T20:10:25,685 [ERROR] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:10:25,685 [ERROR] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:10:25,685 [ERROR] W-9002-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:10:25,685 [ERROR] W-9003-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:10:25,685 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:10:25,686 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:10:25,686 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:10:25,686 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:10:25,687 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:10:25,687 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:10:25,687 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:10:25,687 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:10:25,687 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:10:25,687 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:10:25,688 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:10:25,688 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:10:25,688 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:10:25,688 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:10:25,689 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:10:25,689 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:10:25,689 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:10:25,689 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:10:25,689 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:10:25,689 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:10:25,689 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:10:25,689 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:10:25,689 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:10:25,690 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:10:25,690 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:10:25,690 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:10:25,690 [WARN ] W-9001-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:10:25,690 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:10:25,690 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:10:25,690 [WARN ] W-9005-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:10:25,692 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:10:25,692 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:10:25,692 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:10:25,693 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:10:25,693 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:10:25,693 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:10:25,693 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:10:25,693 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:10:25,693 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:10:25,694 [WARN ] W-9004-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:10:25,699 [INFO ] W-9007-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-mnist_1.0-stderr
2022-02-26T20:10:25,700 [INFO ] W-9001-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-mnist_1.0-stderr
2022-02-26T20:10:25,699 [INFO ] W-9007-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-mnist_1.0-stderr
2022-02-26T20:10:25,700 [INFO ] W-9001-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-mnist_1.0-stdout
2022-02-26T20:10:25,699 [INFO ] W-9007-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-mnist_1.0-stdout
2022-02-26T20:10:25,700 [INFO ] W-9001-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-mnist_1.0-stderr
2022-02-26T20:10:25,701 [WARN ] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-mnist_1.0-stderr
2022-02-26T20:10:25,700 [INFO ] W-9001-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-mnist_1.0-stdout
2022-02-26T20:10:25,701 [WARN ] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-mnist_1.0-stderr
2022-02-26T20:10:25,701 [WARN ] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-mnist_1.0-stderr
2022-02-26T20:10:25,701 [WARN ] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-mnist_1.0-stdout
2022-02-26T20:10:25,699 [INFO ] W-9007-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-mnist_1.0-stdout
2022-02-26T20:10:25,701 [WARN ] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-mnist_1.0-stdout
2022-02-26T20:10:25,701 [WARN ] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-mnist_1.0-stderr
2022-02-26T20:10:25,701 [WARN ] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-mnist_1.0-stdout
2022-02-26T20:10:25,701 [WARN ] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-mnist_1.0-stdout
2022-02-26T20:10:25,701 [ERROR] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:10:25,702 [ERROR] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:10:25,701 [ERROR] W-9007-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:10:25,702 [ERROR] W-9001-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:10:25,704 [INFO ] W-9004-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-mnist_1.0-stderr
2022-02-26T20:10:25,704 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:10:25,704 [INFO ] W-9005-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-mnist_1.0-stderr
2022-02-26T20:10:25,704 [INFO ] W-9004-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-mnist_1.0-stderr
2022-02-26T20:10:25,704 [INFO ] W-9005-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-mnist_1.0-stderr
2022-02-26T20:10:25,704 [INFO ] W-9004-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-mnist_1.0-stdout
2022-02-26T20:10:25,704 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:10:25,704 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:10:25,704 [INFO ] W-9005-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-mnist_1.0-stdout
2022-02-26T20:10:25,705 [WARN ] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-mnist_1.0-stderr
2022-02-26T20:10:25,706 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:10:25,706 [WARN ] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-mnist_1.0-stderr
2022-02-26T20:10:25,705 [WARN ] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-mnist_1.0-stderr
2022-02-26T20:10:25,704 [INFO ] W-9005-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-mnist_1.0-stdout
2022-02-26T20:10:25,706 [WARN ] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-mnist_1.0-stderr
2022-02-26T20:10:25,704 [INFO ] W-9004-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-mnist_1.0-stdout
2022-02-26T20:10:25,704 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:10:25,706 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:10:25,706 [WARN ] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-mnist_1.0-stdout
2022-02-26T20:10:25,706 [WARN ] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-mnist_1.0-stdout
2022-02-26T20:10:25,706 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:10:25,706 [WARN ] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-mnist_1.0-stdout
2022-02-26T20:10:25,706 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:10:25,706 [WARN ] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-mnist_1.0-stdout
2022-02-26T20:10:25,706 [ERROR] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:10:25,706 [ERROR] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:10:25,706 [ERROR] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:10:25,706 [ERROR] W-9005-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:10:25,706 [ERROR] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:10:25,706 [ERROR] W-9004-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:10:25,933 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:10:25,933 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:10:25,984 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906225
2022-02-26T20:10:25,985 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:256.03100967407227|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906225
2022-02-26T20:10:25,985 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906225
2022-02-26T20:10:25,986 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906225
2022-02-26T20:10:25,986 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5131.2578125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906225
2022-02-26T20:10:25,986 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8235.28125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906225
2022-02-26T20:10:25,986 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:68.7|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906225
2022-02-26T20:10:46,248 [INFO ] W-9004-mnist_1.0 ACCESS_LOG - /127.0.0.1:54725 "GET /ping HTTP/1.1" 200 5
2022-02-26T20:10:46,249 [INFO ] W-9004-mnist_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:null
2022-02-26T20:10:58,456 [INFO ] W-9004-mnist_1.0 ACCESS_LOG - /127.0.0.1:54730 "GET /ping HTTP/1.1" 200 0
2022-02-26T20:10:58,456 [INFO ] W-9004-mnist_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:null
2022-02-26T20:11:00,184 [INFO ] W-9004-mnist_1.0 ACCESS_LOG - /127.0.0.1:54732 "GET /ping HTTP/1.1" 200 0
2022-02-26T20:11:00,185 [INFO ] W-9004-mnist_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:null
2022-02-26T20:11:01,202 [INFO ] W-9004-mnist_1.0 ACCESS_LOG - /127.0.0.1:54734 "GET /ping HTTP/1.1" 200 0
2022-02-26T20:11:01,202 [INFO ] W-9004-mnist_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:null
2022-02-26T20:11:02,300 [INFO ] W-9004-mnist_1.0 ACCESS_LOG - /127.0.0.1:54736 "GET /ping HTTP/1.1" 200 0
2022-02-26T20:11:02,300 [INFO ] W-9004-mnist_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:null
2022-02-26T20:11:03,079 [INFO ] W-9004-mnist_1.0 ACCESS_LOG - /127.0.0.1:54738 "GET /ping HTTP/1.1" 200 0
2022-02-26T20:11:03,079 [INFO ] W-9004-mnist_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:null
2022-02-26T20:11:03,804 [INFO ] W-9004-mnist_1.0 ACCESS_LOG - /127.0.0.1:54740 "GET /ping HTTP/1.1" 200 0
2022-02-26T20:11:03,804 [INFO ] W-9004-mnist_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:null
2022-02-26T20:11:04,483 [INFO ] W-9004-mnist_1.0 ACCESS_LOG - /127.0.0.1:54742 "GET /ping HTTP/1.1" 200 0
2022-02-26T20:11:04,483 [INFO ] W-9004-mnist_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:null
2022-02-26T20:11:06,913 [INFO ] W-9004-mnist_1.0 ACCESS_LOG - /127.0.0.1:54745 "GET /ping HTTP/1.1" 200 0
2022-02-26T20:11:06,913 [INFO ] W-9004-mnist_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:null
2022-02-26T20:11:09,444 [INFO ] W-9004-mnist_1.0 ACCESS_LOG - /127.0.0.1:54751 "GET /ping HTTP/1.1" 200 0
2022-02-26T20:11:09,444 [INFO ] W-9004-mnist_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:null
2022-02-26T20:11:25,934 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:11:25,934 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:11:26,041 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906286
2022-02-26T20:11:26,042 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:256.0220069885254|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906286
2022-02-26T20:11:26,042 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906286
2022-02-26T20:11:26,043 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906286
2022-02-26T20:11:26,043 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5122.95703125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906286
2022-02-26T20:11:26,043 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8222.59375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906286
2022-02-26T20:11:26,043 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:68.7|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906286
2022-02-26T20:12:25,933 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:12:25,933 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:12:25,983 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906345
2022-02-26T20:12:25,983 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:256.0282974243164|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906345
2022-02-26T20:12:25,983 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906345
2022-02-26T20:12:25,984 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906345
2022-02-26T20:12:25,984 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5192.3359375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906345
2022-02-26T20:12:25,984 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8154.96875|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906345
2022-02-26T20:12:25,984 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:68.3|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906345
2022-02-26T20:13:25,934 [WARN ] pool-3-thread-2 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:13:25,934 [WARN ] pool-3-thread-2 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:13:26,044 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906406
2022-02-26T20:13:26,045 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:256.0274772644043|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906406
2022-02-26T20:13:26,045 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906406
2022-02-26T20:13:26,046 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906406
2022-02-26T20:13:26,046 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:5031.234375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906406
2022-02-26T20:13:26,046 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:8277.4453125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906406
2022-02-26T20:13:26,046 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:69.3|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906406
2022-02-26T20:14:25,931 [WARN ] pool-3-thread-2 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:14:25,931 [WARN ] pool-3-thread-2 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:14:25,989 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906465
2022-02-26T20:14:25,989 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:256.02541732788086|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906465
2022-02-26T20:14:25,989 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906465
2022-02-26T20:14:25,989 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906465
2022-02-26T20:14:25,989 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:5251.74609375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906465
2022-02-26T20:14:25,989 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:8070.5078125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906465
2022-02-26T20:14:25,989 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:67.9|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906465
2022-02-26T20:15:25,929 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:15:25,929 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:15:26,031 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906526
2022-02-26T20:15:26,032 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:256.01725006103516|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906526
2022-02-26T20:15:26,032 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906526
2022-02-26T20:15:26,032 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906526
2022-02-26T20:15:26,032 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5216.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906526
2022-02-26T20:15:26,032 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8103.046875|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906526
2022-02-26T20:15:26,032 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:68.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906526
2022-02-26T20:16:25,930 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:16:25,930 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:16:26,029 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906586
2022-02-26T20:16:26,030 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:256.03210830688477|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906586
2022-02-26T20:16:26,030 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906586
2022-02-26T20:16:26,030 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906586
2022-02-26T20:16:26,030 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:4918.70703125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906586
2022-02-26T20:16:26,031 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8419.8828125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906586
2022-02-26T20:16:26,031 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:70.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906586
2022-02-26T20:16:36,110 [INFO ] pool-2-thread-9 ACCESS_LOG - /127.0.0.1:54779 "GET /ping HTTP/1.1" 200 1
2022-02-26T20:16:36,111 [INFO ] pool-2-thread-9 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:null
2022-02-26T20:16:53,247 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-26T20:16:53,247 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-26T20:16:53,522 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages
Current directory: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist
Temp directory: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4096 M
Python executable: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python
Config file: ./model-server/config.properties
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Initial Models: mnist=mnist.mar
Log dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Metrics dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Model config: N/A
2022-02-26T20:16:53,522 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages
Current directory: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist
Temp directory: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4096 M
Python executable: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python
Config file: ./model-server/config.properties
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Initial Models: mnist=mnist.mar
Log dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Metrics dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Model config: N/A
2022-02-26T20:16:53,530 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-26T20:16:53,530 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-26T20:16:53,548 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnist.mar
2022-02-26T20:16:53,548 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnist.mar
2022-02-26T20:16:53,703 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnist
2022-02-26T20:16:53,703 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnist
2022-02-26T20:16:53,703 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnist
2022-02-26T20:16:53,703 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnist
2022-02-26T20:16:53,704 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnist loaded.
2022-02-26T20:16:53,704 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnist loaded.
2022-02-26T20:16:53,704 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnist, count: 1
2022-02-26T20:16:53,704 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnist, count: 1
2022-02-26T20:16:53,716 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:16:53,716 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:16:53,718 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2022-02-26T20:16:53,718 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2022-02-26T20:16:53,783 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-26T20:16:53,783 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-26T20:16:53,783 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2022-02-26T20:16:53,783 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2022-02-26T20:16:53,784 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-26T20:16:53,784 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-26T20:16:53,784 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2022-02-26T20:16:53,784 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2022-02-26T20:16:53,785 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-26T20:16:53,785 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-26T20:16:53,803 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:16:53,803 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2022-02-26T20:16:53,804 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2022-02-26T20:16:53,804 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 16, in <module>
2022-02-26T20:16:53,804 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -     from ts.service import Service
2022-02-26T20:16:53,804 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 14, in <module>
2022-02-26T20:16:53,805 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2022-02-26T20:16:53,805 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 15, in <module>
2022-02-26T20:16:53,805 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -     import torch
2022-02-26T20:16:53,805 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2022-02-26T20:16:53,810 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:16:53,810 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:16:53,810 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:16:53,810 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:16:53,811 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:16:53,811 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:16:53,811 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:16:53,811 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:16:53,811 [ERROR] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:16:53,811 [ERROR] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:16:54,011 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:16:54,011 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:16:54,063 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906614
2022-02-26T20:16:54,063 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:256.0300407409668|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906614
2022-02-26T20:16:54,064 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906614
2022-02-26T20:16:54,064 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906614
2022-02-26T20:16:54,064 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5398.078125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906614
2022-02-26T20:16:54,064 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8062.58203125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906614
2022-02-26T20:16:54,064 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:67.1|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906614
2022-02-26T20:17:45,174 [INFO ] W-9000-mnist_1.0 ACCESS_LOG - /127.0.0.1:54788 "GET /ping HTTP/1.1" 200 4
2022-02-26T20:17:45,174 [INFO ] W-9000-mnist_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:null
2022-02-26T20:17:47,226 [INFO ] W-9000-mnist_1.0 ACCESS_LOG - /127.0.0.1:54790 "GET /ping HTTP/1.1" 200 0
2022-02-26T20:17:47,226 [INFO ] W-9000-mnist_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:null
2022-02-26T20:17:54,011 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:17:54,011 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:17:54,060 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906674
2022-02-26T20:17:54,061 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:256.0294494628906|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906674
2022-02-26T20:17:54,061 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906674
2022-02-26T20:17:54,061 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906674
2022-02-26T20:17:54,062 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5350.68359375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906674
2022-02-26T20:17:54,062 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8110.0234375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906674
2022-02-26T20:17:54,062 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:67.3|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906674
2022-02-26T20:18:54,013 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:18:54,013 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:18:54,068 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906734
2022-02-26T20:18:54,069 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:256.0246238708496|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906734
2022-02-26T20:18:54,069 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906734
2022-02-26T20:18:54,069 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906734
2022-02-26T20:18:54,069 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5356.05078125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906734
2022-02-26T20:18:54,069 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8105.55859375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906734
2022-02-26T20:18:54,069 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:67.3|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645906734
2022-02-26T20:27:19,152 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-26T20:27:19,152 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-26T20:27:19,589 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages
Current directory: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist
Temp directory: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4096 M
Python executable: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python
Config file: ./model-server/config.properties
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Initial Models: mnist=mnist.mar
Log dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Metrics dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Model config: N/A
2022-02-26T20:27:19,589 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages
Current directory: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist
Temp directory: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4096 M
Python executable: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python
Config file: ./model-server/config.properties
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Initial Models: mnist=mnist.mar
Log dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Metrics dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Model config: N/A
2022-02-26T20:27:19,604 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-26T20:27:19,604 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-26T20:27:19,640 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnist.mar
2022-02-26T20:27:19,640 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnist.mar
2022-02-26T20:27:19,855 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnist
2022-02-26T20:27:19,855 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnist
2022-02-26T20:27:19,855 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnist
2022-02-26T20:27:19,855 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnist
2022-02-26T20:27:19,856 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnist loaded.
2022-02-26T20:27:19,856 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnist loaded.
2022-02-26T20:27:19,856 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnist, count: 1
2022-02-26T20:27:19,856 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnist, count: 1
2022-02-26T20:27:19,873 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:27:19,873 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:27:19,876 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2022-02-26T20:27:19,876 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2022-02-26T20:27:19,956 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-26T20:27:19,956 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-26T20:27:19,956 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2022-02-26T20:27:19,956 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2022-02-26T20:27:19,957 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-26T20:27:19,957 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-26T20:27:19,958 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2022-02-26T20:27:19,958 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2022-02-26T20:27:19,958 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-26T20:27:19,958 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-26T20:27:20,223 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:27:20,223 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:27:20,278 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907240
2022-02-26T20:27:20,279 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:255.31689071655273|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907240
2022-02-26T20:27:20,279 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907240
2022-02-26T20:27:20,279 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907240
2022-02-26T20:27:20,280 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5293.2265625|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907240
2022-02-26T20:27:20,280 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8194.95703125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907240
2022-02-26T20:27:20,280 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:67.7|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907240
2022-02-26T20:27:25,473 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:27:25,475 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]32782
2022-02-26T20:27:25,475 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:27:25,475 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:27:25,475 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change null -> WORKER_STARTED
2022-02-26T20:27:25,475 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change null -> WORKER_STARTED
2022-02-26T20:27:25,482 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:27:25,482 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:27:25,490 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:27:25,492 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907245492
2022-02-26T20:27:25,492 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907245492
2022-02-26T20:27:25,528 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:27:25,537 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:27:25,537 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:27:25,538 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:27:25,538 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:27:25,537 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG - /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/torch/package/_directory_reader.py:17: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:68.)
2022-02-26T20:27:25,538 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:27:25,538 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}
2022-02-26T20:27:25,538 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:27:25,538 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:27:25,539 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:27:25,539 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:27:25,539 [INFO ] KQueueEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:27:25,539 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:27:25,539 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:27:25,539 [INFO ] KQueueEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:27:25,540 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:27:25,540 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:27:25,540 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:27:25,540 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:27:25,540 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:27:25,540 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/private/var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/models/0cfe6011a2e44b698db0e0d3ff6aac08/mnist_handler.py", line 1, in <module>
2022-02-26T20:27:25,540 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from torchvision import transforms
2022-02-26T20:27:25,540 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'torchvision'
2022-02-26T20:27:25,541 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2022-02-26T20:27:25,541 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-02-26T20:27:25,541 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2022-02-26T20:27:25,541 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:27:25,541 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 189, in <module>
2022-02-26T20:27:25,541 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-26T20:27:25,542 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 161, in run_server
2022-02-26T20:27:25,542 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-26T20:27:25,542 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 123, in handle_connection
2022-02-26T20:27:25,542 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-26T20:27:25,542 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 95, in load_model
2022-02-26T20:27:25,543 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service = model_loader.load(model_name, model_dir, handler, gpu,
2022-02-26T20:27:25,543 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 85, in load
2022-02-26T20:27:25,543 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-02-26T20:27:25,543 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 128, in _load_default_handler
2022-02-26T20:27:25,543 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, 'ts.torch_handler')
2022-02-26T20:27:25,543 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:27:25,544 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:27:25,544 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:27:25,544 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:27:25,544 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 961, in _find_and_load_unlocked
2022-02-26T20:27:25,544 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:27:25,544 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:27:25,545 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:27:25,545 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-02-26T20:27:25,545 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.mnist_handler'
2022-02-26T20:27:25,540 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:27:25,540 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:27:25,555 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:27:25,555 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:27:25,555 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:27:25,555 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:27:25,555 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:27:25,555 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:27:25,556 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:27:25,555 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:27:25,555 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:27:25,556 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:27:25,556 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:27:25,556 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:27:25,557 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-26T20:27:25,557 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-26T20:27:26,560 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:27:26,560 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:27:27,090 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:27:27,091 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]32785
2022-02-26T20:27:27,091 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:27:27,091 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:27:27,091 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:27:27,091 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:27:27,091 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:27:27,091 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:27:27,093 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907247093
2022-02-26T20:27:27,093 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907247093
2022-02-26T20:27:27,093 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:27:27,112 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:27:27,115 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:27:27,116 [INFO ] KQueueEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:27:27,116 [INFO ] KQueueEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:27:27,116 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:27:27,116 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG - /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/torch/package/_directory_reader.py:17: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:68.)
2022-02-26T20:27:27,117 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:27:27,117 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}
2022-02-26T20:27:27,117 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:27:27,117 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:27:27,117 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:27:27,117 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:27:27,117 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:27:27,118 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:27:27,117 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:27:27,118 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:27:27,118 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:27:27,118 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:27:27,118 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:27:27,118 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:27:27,118 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:27:27,118 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:27:27,118 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:27:27,118 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:27:27,119 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:27:27,118 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:27:27,119 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:27:27,119 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:27:27,119 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:27:27,119 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:27:27,119 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:27:27,119 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-26T20:27:27,119 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-26T20:27:27,119 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:27:27,119 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:27:28,124 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:27:28,124 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:27:28,589 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:27:28,590 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]32788
2022-02-26T20:27:28,590 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:27:28,590 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:27:28,590 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:27:28,590 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:27:28,590 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:27:28,590 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:27:28,592 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907248592
2022-02-26T20:27:28,592 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:27:28,592 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907248592
2022-02-26T20:27:28,608 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:27:28,611 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:27:28,611 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:27:28,611 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:27:28,611 [INFO ] KQueueEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:27:28,611 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:27:28,611 [INFO ] KQueueEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:27:28,611 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG - /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/torch/package/_directory_reader.py:17: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:68.)
2022-02-26T20:27:28,612 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:27:28,612 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}
2022-02-26T20:27:28,612 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:27:28,612 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:27:28,612 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:27:28,612 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:27:28,612 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:27:28,612 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:27:28,612 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:27:28,613 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:27:28,613 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:27:28,613 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:27:28,613 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:27:28,613 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:27:28,613 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:27:28,613 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:27:28,613 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:27:28,613 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:27:28,613 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:27:28,614 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:27:28,614 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:27:28,614 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:27:28,613 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:27:28,614 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:27:28,614 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-02-26T20:27:28,614 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-02-26T20:27:28,614 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:27:28,614 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:27:30,618 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:27:30,618 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:27:31,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:27:31,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]32791
2022-02-26T20:27:31,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:27:31,125 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:27:31,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:27:31,125 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:27:31,126 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:27:31,126 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:27:31,127 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907251127
2022-02-26T20:27:31,127 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907251127
2022-02-26T20:27:31,127 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:27:31,143 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:27:31,145 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:27:31,146 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:27:31,147 [INFO ] KQueueEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:27:31,147 [INFO ] KQueueEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:27:31,146 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG - /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/torch/package/_directory_reader.py:17: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:68.)
2022-02-26T20:27:31,147 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:27:31,147 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:27:31,147 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:27:31,147 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:27:31,147 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:27:31,147 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:27:31,148 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:27:31,147 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:27:31,147 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}
2022-02-26T20:27:31,148 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:27:31,148 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:27:31,148 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:27:31,148 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:27:31,148 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:27:31,149 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:27:31,149 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:27:31,149 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:27:31,149 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:27:31,149 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:27:31,150 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:27:31,150 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:27:31,150 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:27:31,150 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:27:31,150 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-02-26T20:27:31,150 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:27:31,150 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-02-26T20:27:31,150 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:27:34,154 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:27:34,154 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:27:34,616 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:27:34,617 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]32796
2022-02-26T20:27:34,618 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:27:34,618 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:27:34,618 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:27:34,618 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:27:34,618 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:27:34,618 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:27:34,619 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907254619
2022-02-26T20:27:34,619 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907254619
2022-02-26T20:27:34,619 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:27:34,637 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:27:34,639 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:27:34,639 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:27:34,639 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:27:34,639 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:27:34,639 [INFO ] KQueueEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:27:34,639 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:27:34,639 [INFO ] KQueueEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:27:34,640 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:27:34,640 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:27:34,639 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG - /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/torch/package/_directory_reader.py:17: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:68.)
2022-02-26T20:27:34,640 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:27:34,640 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:27:34,640 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}
2022-02-26T20:27:34,640 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:27:34,640 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:27:34,640 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:27:34,640 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:27:34,640 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:27:34,640 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:27:34,640 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:27:34,640 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:27:34,640 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:27:34,640 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:27:34,640 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:27:34,641 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:27:34,641 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:27:34,641 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:27:34,641 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:27:34,641 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:27:34,641 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:27:34,641 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/private/var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/models/0cfe6011a2e44b698db0e0d3ff6aac08/mnist_handler.py", line 1, in <module>
2022-02-26T20:27:34,641 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:27:34,641 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:27:34,641 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from torchvision import transforms
2022-02-26T20:27:34,641 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-02-26T20:27:34,641 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:27:34,641 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-02-26T20:27:34,641 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:27:39,642 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:27:39,642 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:27:40,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:27:40,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]32799
2022-02-26T20:27:40,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:27:40,126 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:27:40,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:27:40,126 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:27:40,126 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:27:40,126 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:27:40,127 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907260127
2022-02-26T20:27:40,127 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:27:40,127 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907260127
2022-02-26T20:27:40,144 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:27:40,146 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:27:40,146 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:27:40,146 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:27:40,146 [INFO ] KQueueEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:27:40,146 [INFO ] KQueueEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:27:40,146 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:27:40,146 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG - /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/torch/package/_directory_reader.py:17: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:68.)
2022-02-26T20:27:40,146 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:27:40,146 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:27:40,146 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:27:40,146 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}
2022-02-26T20:27:40,147 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:27:40,147 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:27:40,147 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:27:40,147 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:27:40,147 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:27:40,147 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:27:40,147 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:27:40,147 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:27:40,147 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:27:40,147 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:27:40,147 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:27:40,147 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:27:40,148 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:27:40,148 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:27:40,148 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:27:40,148 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:27:40,148 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:27:40,148 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:27:40,148 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:27:40,148 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:27:40,148 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-02-26T20:27:40,148 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-02-26T20:27:40,148 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:27:40,148 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:27:48,150 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:27:48,150 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:27:48,615 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:27:48,616 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]32801
2022-02-26T20:27:48,616 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:27:48,616 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:27:48,616 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:27:48,616 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:27:48,617 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:27:48,617 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:27:48,618 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907268618
2022-02-26T20:27:48,618 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907268618
2022-02-26T20:27:48,618 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:27:48,647 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:27:48,650 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:27:48,650 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:27:48,650 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:27:48,650 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:27:48,650 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:27:48,650 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:27:48,650 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG - /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/torch/package/_directory_reader.py:17: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:68.)
2022-02-26T20:27:48,651 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:27:48,650 [INFO ] KQueueEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:27:48,651 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}
2022-02-26T20:27:48,651 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:27:48,650 [INFO ] KQueueEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:27:48,651 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:27:48,651 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:27:48,651 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:27:48,651 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:27:48,651 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:27:48,651 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:27:48,651 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:27:48,651 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:27:48,651 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:27:48,651 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:27:48,651 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:27:48,651 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:27:48,651 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:27:48,651 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:27:48,651 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/private/var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/models/0cfe6011a2e44b698db0e0d3ff6aac08/mnist_handler.py", line 1, in <module>
2022-02-26T20:27:48,652 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from torchvision import transforms
2022-02-26T20:27:48,652 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:27:48,652 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'torchvision'
2022-02-26T20:27:48,652 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:27:48,652 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:27:48,652 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:27:48,652 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2022-02-26T20:27:48,652 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:27:48,652 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:27:48,652 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-02-26T20:27:48,652 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-02-26T20:27:48,652 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:27:48,652 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-02-26T20:27:48,652 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:28:01,653 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:28:01,653 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:28:02,153 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:28:02,154 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]32805
2022-02-26T20:28:02,154 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:28:02,154 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:28:02,154 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:28:02,154 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:28:02,154 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:28:02,154 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:28:02,155 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907282155
2022-02-26T20:28:02,155 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:28:02,155 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907282155
2022-02-26T20:28:02,176 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:28:02,178 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:28:02,178 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:28:02,178 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:28:02,178 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:28:02,178 [INFO ] KQueueEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:28:02,178 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:28:02,178 [INFO ] KQueueEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:28:02,178 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:28:02,178 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:28:02,178 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG - /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/torch/package/_directory_reader.py:17: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:68.)
2022-02-26T20:28:02,178 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:28:02,178 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:28:02,178 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}
2022-02-26T20:28:02,178 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:28:02,178 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:28:02,178 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:28:02,179 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:28:02,178 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:28:02,179 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:28:02,179 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:28:02,179 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:28:02,179 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:28:02,179 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:28:02,179 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:28:02,179 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:28:02,179 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:28:02,179 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/private/var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/models/0cfe6011a2e44b698db0e0d3ff6aac08/mnist_handler.py", line 1, in <module>
2022-02-26T20:28:02,179 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:28:02,179 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:28:02,179 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from torchvision import transforms
2022-02-26T20:28:02,179 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:28:02,179 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:28:02,179 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'torchvision'
2022-02-26T20:28:02,180 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:28:02,180 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:28:02,180 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2022-02-26T20:28:02,180 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-02-26T20:28:02,180 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-02-26T20:28:02,180 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:28:02,180 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:28:20,286 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907300
2022-02-26T20:28:20,287 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:255.31819534301758|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907300
2022-02-26T20:28:20,287 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907300
2022-02-26T20:28:20,287 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907300
2022-02-26T20:28:20,287 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:5379.99609375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907300
2022-02-26T20:28:20,287 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:8155.5|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907300
2022-02-26T20:28:20,287 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:67.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907300
2022-02-26T20:28:23,184 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:28:23,184 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:28:23,793 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:28:23,794 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]32810
2022-02-26T20:28:23,794 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:28:23,794 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:28:23,794 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:28:23,794 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:28:23,794 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:28:23,794 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:28:23,795 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907303795
2022-02-26T20:28:23,795 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907303795
2022-02-26T20:28:23,795 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:28:23,817 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:28:23,819 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:28:23,819 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:28:23,819 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:28:23,819 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:28:23,819 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:28:23,819 [INFO ] KQueueEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:28:23,819 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:28:23,819 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:28:23,819 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG - /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/torch/package/_directory_reader.py:17: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:68.)
2022-02-26T20:28:23,819 [INFO ] KQueueEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:28:23,820 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:28:23,820 [WARN ] W-9000-mnist_1.0-stderr MODEL_LOG -   _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}
2022-02-26T20:28:23,820 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:28:23,820 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:28:23,820 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:28:23,820 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:28:23,820 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:28:23,820 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:28:23,820 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:28:23,820 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:28:23,820 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:28:23,820 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:28:23,820 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:28:23,820 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:28:23,820 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:28:23,820 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:28:23,820 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/private/var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/models/0cfe6011a2e44b698db0e0d3ff6aac08/mnist_handler.py", line 1, in <module>
2022-02-26T20:28:23,820 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from torchvision import transforms
2022-02-26T20:28:23,820 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'torchvision'
2022-02-26T20:28:23,821 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:28:23,821 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:28:23,821 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:28:23,821 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:28:23,821 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2022-02-26T20:28:23,821 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:28:23,821 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:28:23,821 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-02-26T20:28:23,821 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-02-26T20:28:23,821 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-02-26T20:28:23,821 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:28:23,821 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:33:28,571 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-26T20:33:28,571 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-26T20:33:29,094 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages
Current directory: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist
Temp directory: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4096 M
Python executable: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python
Config file: ./model-server/config.properties
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Initial Models: mnist=mnist.mar
Log dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Metrics dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Model config: N/A
2022-02-26T20:33:29,094 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages
Current directory: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist
Temp directory: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4096 M
Python executable: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python
Config file: ./model-server/config.properties
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Initial Models: mnist=mnist.mar
Log dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Metrics dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Model config: N/A
2022-02-26T20:33:29,137 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-26T20:33:29,137 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-26T20:33:29,157 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnist.mar
2022-02-26T20:33:29,157 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnist.mar
2022-02-26T20:33:29,310 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnist
2022-02-26T20:33:29,310 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnist
2022-02-26T20:33:29,311 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnist
2022-02-26T20:33:29,311 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnist
2022-02-26T20:33:29,311 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnist loaded.
2022-02-26T20:33:29,311 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnist loaded.
2022-02-26T20:33:29,311 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnist, count: 1
2022-02-26T20:33:29,311 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnist, count: 1
2022-02-26T20:33:29,322 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:33:29,322 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:33:29,324 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2022-02-26T20:33:29,324 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2022-02-26T20:33:29,395 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-26T20:33:29,395 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-26T20:33:29,395 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2022-02-26T20:33:29,395 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2022-02-26T20:33:29,396 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-26T20:33:29,396 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-26T20:33:29,396 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2022-02-26T20:33:29,396 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2022-02-26T20:33:29,397 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-26T20:33:29,397 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-26T20:33:29,655 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:33:29,655 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:33:29,722 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907609
2022-02-26T20:33:29,723 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:255.1982307434082|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907609
2022-02-26T20:33:29,723 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907609
2022-02-26T20:33:29,723 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907609
2022-02-26T20:33:29,723 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5381.75|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907609
2022-02-26T20:33:29,723 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8352.0546875|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907609
2022-02-26T20:33:29,724 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:67.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907609
2022-02-26T20:33:38,988 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:33:38,989 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]32920
2022-02-26T20:33:38,989 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:33:38,989 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:33:38,989 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change null -> WORKER_STARTED
2022-02-26T20:33:38,989 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change null -> WORKER_STARTED
2022-02-26T20:33:38,992 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:33:38,992 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:33:38,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:33:38,998 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907618998
2022-02-26T20:33:38,998 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907618998
2022-02-26T20:33:39,017 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:33:39,020 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:33:39,021 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:33:39,021 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:33:39,021 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:33:39,021 [INFO ] KQueueEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:33:39,021 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:33:39,021 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:33:39,021 [INFO ] KQueueEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:33:39,021 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:33:39,022 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:33:39,022 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:33:39,022 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:33:39,022 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:33:39,022 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:33:39,022 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:33:39,023 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:33:39,023 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:33:39,023 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:33:39,023 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/private/var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/models/33e4379f412247909b8a6e25a7d1d62d/mnist_handler.py", line 1, in <module>
2022-02-26T20:33:39,024 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from torchvision import transforms
2022-02-26T20:33:39,024 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'torchvision'
2022-02-26T20:33:39,024 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2022-02-26T20:33:39,025 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-02-26T20:33:39,025 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2022-02-26T20:33:39,025 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:33:39,025 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 189, in <module>
2022-02-26T20:33:39,025 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-26T20:33:39,025 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 161, in run_server
2022-02-26T20:33:39,026 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-26T20:33:39,026 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 123, in handle_connection
2022-02-26T20:33:39,026 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-26T20:33:39,026 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 95, in load_model
2022-02-26T20:33:39,026 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service = model_loader.load(model_name, model_dir, handler, gpu,
2022-02-26T20:33:39,026 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 85, in load
2022-02-26T20:33:39,027 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-02-26T20:33:39,027 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 128, in _load_default_handler
2022-02-26T20:33:39,027 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, 'ts.torch_handler')
2022-02-26T20:33:39,027 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:33:39,027 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:33:39,027 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:33:39,028 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:33:39,028 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 961, in _find_and_load_unlocked
2022-02-26T20:33:39,028 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:33:39,028 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:33:39,029 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:33:39,029 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-02-26T20:33:39,029 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.mnist_handler'
2022-02-26T20:33:39,022 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:33:39,022 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:33:39,034 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:33:39,034 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:33:39,034 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:33:39,034 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:33:39,035 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:33:39,035 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:33:39,035 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:33:39,035 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:33:39,035 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:33:39,035 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:33:39,035 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:33:39,035 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:33:39,035 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-26T20:33:39,035 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-26T20:33:40,037 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:33:40,037 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:33:40,639 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:33:40,640 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]32926
2022-02-26T20:33:40,640 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:33:40,640 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:33:40,640 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:33:40,640 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:33:40,640 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:33:40,640 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:33:40,641 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907620641
2022-02-26T20:33:40,641 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907620641
2022-02-26T20:33:40,642 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:33:40,667 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:33:40,671 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:33:40,672 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:33:40,672 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:33:40,672 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:33:40,672 [INFO ] KQueueEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:33:40,673 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:33:40,673 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:33:40,672 [INFO ] KQueueEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:33:40,673 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:33:40,673 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:33:40,673 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:33:40,673 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:33:40,673 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:33:40,674 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:33:40,674 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:33:40,674 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:33:40,674 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:33:40,675 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:33:40,675 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:33:40,675 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:33:40,675 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:33:40,675 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:33:40,675 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:33:40,675 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:33:40,675 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/private/var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/models/33e4379f412247909b8a6e25a7d1d62d/mnist_handler.py", line 1, in <module>
2022-02-26T20:33:40,676 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from torchvision import transforms
2022-02-26T20:33:40,676 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'torchvision'
2022-02-26T20:33:40,676 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:33:40,676 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2022-02-26T20:33:40,676 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:33:40,676 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:33:40,677 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:33:40,677 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:33:40,676 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:33:40,676 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-02-26T20:33:40,677 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-26T20:33:40,677 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-26T20:33:40,677 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:33:40,677 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:33:41,679 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:33:41,679 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:33:42,220 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:33:42,221 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]32928
2022-02-26T20:33:42,221 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:33:42,221 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:33:42,221 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:33:42,221 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:33:42,221 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:33:42,221 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:33:42,223 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907622223
2022-02-26T20:33:42,223 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907622223
2022-02-26T20:33:42,223 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:33:42,245 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:33:42,246 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:33:42,247 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:33:42,247 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:33:42,247 [INFO ] KQueueEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:33:42,247 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:33:42,247 [INFO ] KQueueEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:33:42,247 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:33:42,247 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:33:42,247 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:33:42,248 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:33:42,248 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:33:42,248 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:33:42,248 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:33:42,248 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:33:42,248 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:33:42,248 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:33:42,248 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:33:42,248 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:33:42,248 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:33:42,248 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:33:42,249 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:33:42,249 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:33:42,249 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:33:42,249 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/private/var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/models/33e4379f412247909b8a6e25a7d1d62d/mnist_handler.py", line 1, in <module>
2022-02-26T20:33:42,249 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from torchvision import transforms
2022-02-26T20:33:42,249 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'torchvision'
2022-02-26T20:33:42,248 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:33:42,249 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2022-02-26T20:33:42,249 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-02-26T20:33:42,249 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2022-02-26T20:33:42,249 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:33:42,250 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 189, in <module>
2022-02-26T20:33:42,250 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:33:42,250 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-26T20:33:42,250 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:33:42,250 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:33:42,250 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:33:42,250 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 161, in run_server
2022-02-26T20:33:42,250 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:33:42,250 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:33:42,250 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-26T20:33:42,251 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-02-26T20:33:42,251 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:33:42,251 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-02-26T20:33:42,251 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:33:44,254 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:33:44,254 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:33:44,850 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:33:44,851 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]32930
2022-02-26T20:33:44,851 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:33:44,851 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:33:44,851 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:33:44,851 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:33:44,851 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:33:44,851 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:33:44,852 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907624852
2022-02-26T20:33:44,852 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:33:44,852 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907624852
2022-02-26T20:33:44,866 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:33:44,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:33:44,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:33:44,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:33:44,868 [INFO ] KQueueEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:33:44,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:33:44,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:33:44,868 [INFO ] KQueueEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:33:44,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:33:44,868 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:33:44,868 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:33:44,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:33:44,869 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:33:44,869 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:33:44,869 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:33:44,869 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:33:44,869 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:33:44,869 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:33:44,869 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:33:44,869 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:33:44,869 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:33:44,869 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:33:44,869 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:33:44,869 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:33:44,870 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:33:44,870 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:33:44,870 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:33:44,870 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:33:44,870 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:33:44,870 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:33:44,870 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/private/var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/models/33e4379f412247909b8a6e25a7d1d62d/mnist_handler.py", line 1, in <module>
2022-02-26T20:33:44,870 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:33:44,870 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:33:44,870 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-02-26T20:33:44,870 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:33:44,870 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-02-26T20:33:47,875 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:33:47,875 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:33:48,403 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:33:48,404 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]32932
2022-02-26T20:33:48,404 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:33:48,404 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:33:48,404 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:33:48,404 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:33:48,404 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:33:48,404 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:33:48,405 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907628405
2022-02-26T20:33:48,405 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:33:48,405 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907628405
2022-02-26T20:33:48,420 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:33:48,422 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:33:48,422 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:33:48,422 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:33:48,422 [INFO ] KQueueEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:33:48,423 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:33:48,422 [INFO ] KQueueEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:33:48,423 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:33:48,423 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:33:48,423 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:33:48,423 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:33:48,423 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:33:48,423 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:33:48,423 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:33:48,423 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:33:48,423 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:33:48,423 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:33:48,423 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:33:48,423 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:33:48,423 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:33:48,423 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:33:48,424 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:33:48,424 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:33:48,424 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:33:48,424 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:33:48,424 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:33:48,424 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:33:48,424 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:33:48,424 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:33:48,424 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:33:48,424 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:33:48,424 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-02-26T20:33:48,424 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:33:48,424 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-02-26T20:33:48,424 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:33:53,427 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:33:53,427 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:33:53,992 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:33:53,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]32935
2022-02-26T20:33:53,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:33:53,994 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:33:53,994 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:33:53,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:33:53,994 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:33:53,994 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:33:53,995 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907633995
2022-02-26T20:33:53,995 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907633995
2022-02-26T20:33:53,995 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:33:54,020 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:33:54,023 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:33:54,023 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:33:54,023 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:33:54,023 [INFO ] KQueueEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:33:54,024 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:33:54,023 [INFO ] KQueueEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:33:54,024 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:33:54,024 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:33:54,024 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:33:54,024 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:33:54,024 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:33:54,024 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:33:54,024 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:33:54,024 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:33:54,024 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:33:54,025 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:33:54,025 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:33:54,025 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:33:54,025 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:33:54,025 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:33:54,025 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:33:54,025 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:33:54,026 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:33:54,026 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:33:54,026 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:33:54,025 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:33:54,026 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:33:54,026 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:33:54,025 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:33:54,026 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:33:54,026 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-02-26T20:33:54,026 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:33:54,026 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-02-26T20:33:54,026 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:34:02,031 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:34:02,031 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:34:02,544 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:34:02,545 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]32938
2022-02-26T20:34:02,545 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:34:02,545 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:34:02,545 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:34:02,545 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:34:02,545 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:34:02,545 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:34:02,546 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907642546
2022-02-26T20:34:02,546 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:34:02,546 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907642546
2022-02-26T20:34:02,570 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:34:02,572 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:34:02,572 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:34:02,572 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:34:02,572 [INFO ] KQueueEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:34:02,572 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:34:02,572 [INFO ] KQueueEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:34:02,572 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:34:02,572 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:34:02,572 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:34:02,572 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:34:02,572 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:34:02,572 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:34:02,572 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:34:02,572 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:34:02,572 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:34:02,573 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:34:02,573 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:34:02,573 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:34:02,573 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:34:02,573 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:34:02,573 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:34:02,573 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:34:02,573 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:34:02,573 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:34:02,573 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:34:02,573 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:34:02,573 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/private/var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/models/33e4379f412247909b8a6e25a7d1d62d/mnist_handler.py", line 1, in <module>
2022-02-26T20:34:02,573 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:34:02,573 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:34:02,573 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:34:02,573 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from torchvision import transforms
2022-02-26T20:34:02,573 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:34:02,574 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-02-26T20:34:02,574 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:34:02,574 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-02-26T20:34:02,574 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:34:15,575 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:34:15,575 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:34:16,109 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:34:16,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]32942
2022-02-26T20:34:16,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:34:16,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:34:16,110 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:34:16,110 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:34:16,110 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:34:16,110 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:34:16,111 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907656111
2022-02-26T20:34:16,111 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:34:16,111 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907656111
2022-02-26T20:34:16,132 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:34:16,133 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:34:16,134 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:34:16,134 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:34:16,134 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:34:16,134 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:34:16,134 [INFO ] KQueueEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:34:16,134 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:34:16,134 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:34:16,134 [INFO ] KQueueEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:34:16,134 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:34:16,134 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:34:16,134 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:34:16,134 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:34:16,134 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:34:16,134 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:34:16,134 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:34:16,134 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:34:16,135 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:34:16,135 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:34:16,135 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:34:16,135 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:34:16,135 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:34:16,135 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:34:16,135 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:34:16,135 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/private/var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/models/33e4379f412247909b8a6e25a7d1d62d/mnist_handler.py", line 1, in <module>
2022-02-26T20:34:16,135 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:34:16,135 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from torchvision import transforms
2022-02-26T20:34:16,135 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:34:16,135 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:34:16,135 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'torchvision'
2022-02-26T20:34:16,135 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:34:16,135 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:34:16,135 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2022-02-26T20:34:16,135 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-02-26T20:34:16,135 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:34:16,135 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:34:16,135 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-02-26T20:34:16,135 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:34:29,708 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907669
2022-02-26T20:34:29,709 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:255.16315460205078|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907669
2022-02-26T20:34:29,709 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907669
2022-02-26T20:34:29,709 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907669
2022-02-26T20:34:29,709 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5404.38671875|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907669
2022-02-26T20:34:29,709 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8258.98828125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907669
2022-02-26T20:34:29,709 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:67.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907669
2022-02-26T20:34:37,141 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:34:37,141 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:34:37,859 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:34:37,860 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]32948
2022-02-26T20:34:37,860 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:34:37,860 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:34:37,860 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:34:37,860 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:34:37,860 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:34:37,860 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:34:37,861 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907677861
2022-02-26T20:34:37,861 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:34:37,861 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907677861
2022-02-26T20:34:37,888 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:34:37,892 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:34:37,892 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:34:37,892 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:34:37,892 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:34:37,892 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:34:37,892 [INFO ] KQueueEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:34:37,892 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:34:37,892 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:34:37,892 [INFO ] KQueueEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:34:37,892 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:34:37,892 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:34:37,892 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:34:37,892 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:34:37,892 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:34:37,893 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:34:37,892 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:34:37,893 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:34:37,892 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:34:37,893 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:34:37,893 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:34:37,893 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:34:37,893 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:34:37,893 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:34:37,893 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:34:37,893 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/private/var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/models/33e4379f412247909b8a6e25a7d1d62d/mnist_handler.py", line 1, in <module>
2022-02-26T20:34:37,893 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from torchvision import transforms
2022-02-26T20:34:37,893 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:34:37,893 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'torchvision'
2022-02-26T20:34:37,893 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:34:37,893 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:34:37,893 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:34:37,893 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2022-02-26T20:34:37,893 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:34:37,893 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-02-26T20:34:37,894 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-02-26T20:34:37,894 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-02-26T20:34:37,893 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:34:37,894 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:34:37,894 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:36:22,914 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-26T20:36:22,914 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-26T20:36:23,189 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages
Current directory: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist
Temp directory: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4096 M
Python executable: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python
Config file: ./model-server/config.properties
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Initial Models: mnist=mnist.mar
Log dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Metrics dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Model config: N/A
2022-02-26T20:36:23,189 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages
Current directory: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist
Temp directory: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4096 M
Python executable: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python
Config file: ./model-server/config.properties
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Initial Models: mnist=mnist.mar
Log dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Metrics dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Model config: N/A
2022-02-26T20:36:23,198 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-26T20:36:23,198 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-26T20:36:23,216 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnist.mar
2022-02-26T20:36:23,216 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnist.mar
2022-02-26T20:36:23,381 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnist
2022-02-26T20:36:23,381 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnist
2022-02-26T20:36:23,381 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnist
2022-02-26T20:36:23,381 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnist
2022-02-26T20:36:23,382 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnist loaded.
2022-02-26T20:36:23,382 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnist loaded.
2022-02-26T20:36:23,382 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnist, count: 1
2022-02-26T20:36:23,382 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnist, count: 1
2022-02-26T20:36:23,394 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:36:23,394 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:36:23,395 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2022-02-26T20:36:23,395 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2022-02-26T20:36:23,469 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-26T20:36:23,469 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-26T20:36:23,470 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2022-02-26T20:36:23,470 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2022-02-26T20:36:23,471 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-26T20:36:23,471 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-26T20:36:23,471 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2022-02-26T20:36:23,471 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2022-02-26T20:36:23,472 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-26T20:36:23,472 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-26T20:36:23,704 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:36:23,704 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:36:23,771 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907783
2022-02-26T20:36:23,772 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:255.15612411499023|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907783
2022-02-26T20:36:23,772 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907783
2022-02-26T20:36:23,773 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907783
2022-02-26T20:36:23,773 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5483.21484375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907783
2022-02-26T20:36:23,773 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8191.19140625|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907783
2022-02-26T20:36:23,773 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:66.5|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907783
2022-02-26T20:36:23,981 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:36:23,982 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]33001
2022-02-26T20:36:23,982 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:36:23,982 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:36:23,983 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change null -> WORKER_STARTED
2022-02-26T20:36:23,983 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change null -> WORKER_STARTED
2022-02-26T20:36:23,988 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:36:23,988 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:36:23,995 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:36:23,997 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907783997
2022-02-26T20:36:23,997 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907783997
2022-02-26T20:36:24,024 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:36:27,579 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:36:27,579 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:36:27,579 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:36:27,579 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:36:27,580 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:36:27,580 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:36:27,580 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:36:27,580 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:36:27,580 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:36:27,580 [INFO ] KQueueEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:36:27,581 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:36:27,580 [INFO ] KQueueEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:36:27,581 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:36:27,581 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:36:27,581 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:36:27,581 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:36:27,581 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:36:27,581 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:36:27,582 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/private/var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/models/d8e5c78a5f2a40899ccf2bc1b24393d3/mnist_handler.py", line 2, in <module>
2022-02-26T20:36:27,582 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-02-26T20:36:27,582 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-02-26T20:36:27,582 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-02-26T20:36:27,582 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-02-26T20:36:27,583 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-02-26T20:36:27,584 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-02-26T20:36:27,584 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2022-02-26T20:36:27,584 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-02-26T20:36:27,584 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2022-02-26T20:36:27,584 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:36:27,584 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 189, in <module>
2022-02-26T20:36:27,585 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-26T20:36:27,585 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 161, in run_server
2022-02-26T20:36:27,585 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-26T20:36:27,585 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 123, in handle_connection
2022-02-26T20:36:27,586 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-26T20:36:27,586 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 95, in load_model
2022-02-26T20:36:27,586 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service = model_loader.load(model_name, model_dir, handler, gpu,
2022-02-26T20:36:27,587 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 85, in load
2022-02-26T20:36:27,587 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-02-26T20:36:27,587 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 128, in _load_default_handler
2022-02-26T20:36:27,587 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, 'ts.torch_handler')
2022-02-26T20:36:27,587 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:36:27,587 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:36:27,588 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:36:27,588 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:36:27,588 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 961, in _find_and_load_unlocked
2022-02-26T20:36:27,588 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:36:27,588 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:36:27,588 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:36:27,588 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-02-26T20:36:27,589 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.mnist_handler'
2022-02-26T20:36:27,581 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:36:27,581 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:36:27,595 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:36:27,595 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:36:27,595 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:36:27,595 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:36:27,595 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:36:27,595 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:36:27,595 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:36:27,595 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:36:27,595 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:36:27,596 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:36:27,595 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:36:27,596 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:36:27,596 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-26T20:36:27,596 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-26T20:36:28,600 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:36:28,600 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:36:29,238 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:36:29,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]33005
2022-02-26T20:36:29,239 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:36:29,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:36:29,239 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:36:29,239 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:36:29,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:36:29,239 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:36:29,240 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907789240
2022-02-26T20:36:29,240 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:36:29,240 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907789240
2022-02-26T20:36:29,256 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:36:29,332 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:36:29,333 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:36:29,333 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:36:29,333 [INFO ] KQueueEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:36:29,333 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:36:29,333 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:36:29,333 [INFO ] KQueueEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:36:29,333 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:36:29,334 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:36:29,334 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:36:29,334 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:36:29,334 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:36:29,334 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:36:29,334 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:36:29,334 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:36:29,334 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:36:29,334 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:36:29,334 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:36:29,334 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:36:29,334 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:36:29,335 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:36:29,335 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:36:29,335 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:36:29,335 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:36:29,335 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:36:29,335 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:36:29,335 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:36:29,335 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-26T20:36:29,335 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:36:29,335 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:36:29,335 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-26T20:36:29,336 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:36:29,336 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:36:30,338 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:36:30,338 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:36:30,899 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:36:30,900 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]33008
2022-02-26T20:36:30,900 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:36:30,900 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:36:30,900 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:36:30,900 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:36:30,900 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:36:30,900 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:36:30,902 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907790902
2022-02-26T20:36:30,902 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907790902
2022-02-26T20:36:30,902 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:36:30,916 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:36:30,995 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:36:30,995 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:36:30,995 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:36:30,995 [INFO ] KQueueEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:36:30,995 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:36:30,995 [INFO ] KQueueEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:36:30,995 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:36:30,995 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:36:30,995 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:36:30,996 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:36:30,996 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:36:30,996 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:36:30,996 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:36:30,996 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:36:30,996 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:36:30,996 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:36:30,996 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:36:30,996 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:36:30,996 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:36:30,996 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:36:30,996 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:36:30,996 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:36:30,997 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:36:30,997 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:36:30,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:36:30,997 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:36:30,997 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:36:30,997 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:36:30,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:36:30,997 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-02-26T20:36:30,997 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:36:30,997 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:36:30,997 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-02-26T20:36:30,997 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:36:32,999 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:36:32,999 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:36:33,539 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:36:33,539 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]33010
2022-02-26T20:36:33,539 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:36:33,539 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:36:33,539 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:36:33,539 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:36:33,540 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:36:33,540 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:36:33,541 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907793541
2022-02-26T20:36:33,541 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907793541
2022-02-26T20:36:33,541 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:36:33,561 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:36:33,639 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:36:33,639 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:36:33,639 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:36:33,639 [INFO ] KQueueEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:36:33,639 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:36:33,639 [INFO ] KQueueEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:36:33,640 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:36:33,640 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:36:33,640 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:36:33,640 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:36:33,640 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:36:33,640 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:36:33,640 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:36:33,640 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:36:33,640 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:36:33,640 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:36:33,640 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:36:33,641 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:36:33,641 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:36:33,641 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:36:33,641 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:36:33,641 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:36:33,641 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:36:33,641 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:36:33,641 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:36:33,641 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:36:33,641 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:36:33,641 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:36:33,641 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:36:33,641 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-02-26T20:36:33,641 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:36:33,641 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-02-26T20:36:33,641 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:36:36,645 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:36:36,645 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:36:37,163 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:36:37,164 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]33012
2022-02-26T20:36:37,164 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:36:37,164 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:36:37,164 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:36:37,164 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:36:37,164 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:36:37,164 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:36:37,166 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907797166
2022-02-26T20:36:37,166 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:36:37,166 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907797166
2022-02-26T20:36:37,180 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:36:37,255 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:36:37,256 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:36:37,256 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:36:37,256 [INFO ] KQueueEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:36:37,256 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:36:37,256 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:36:37,256 [INFO ] KQueueEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:36:37,256 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:36:37,256 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:36:37,256 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:36:37,256 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:36:37,256 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:36:37,256 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:36:37,256 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:36:37,256 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:36:37,257 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:36:37,257 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:36:37,257 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:36:37,257 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:36:37,257 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:36:37,257 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:36:37,257 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:36:37,257 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:36:37,257 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:36:37,257 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:36:37,257 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:36:37,257 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:36:37,257 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:36:37,257 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:36:37,257 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-02-26T20:36:37,257 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:36:37,258 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:36:37,257 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-02-26T20:36:37,258 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:36:42,263 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:36:42,263 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:36:42,859 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:36:42,859 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]33015
2022-02-26T20:36:42,859 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:36:42,859 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:36:42,859 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:36:42,860 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:36:42,860 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:36:42,860 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:36:42,861 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907802861
2022-02-26T20:36:42,861 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907802861
2022-02-26T20:36:42,861 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:36:42,877 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:36:42,961 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:36:42,962 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:36:42,962 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:36:42,962 [INFO ] KQueueEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:36:42,962 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:36:42,962 [INFO ] KQueueEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:36:42,962 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:36:42,962 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:36:42,962 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:36:42,962 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:36:42,962 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:36:42,962 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:36:42,962 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:36:42,963 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:36:42,962 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:36:42,963 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:36:42,963 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:36:42,963 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:36:42,963 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:36:42,963 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:36:42,963 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:36:42,963 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:36:42,963 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:36:42,964 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:36:42,964 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:36:42,964 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:36:42,964 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:36:42,964 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:36:42,964 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:36:42,964 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/private/var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/models/d8e5c78a5f2a40899ccf2bc1b24393d3/mnist_handler.py", line 2, in <module>
2022-02-26T20:36:42,964 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-02-26T20:36:42,964 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:36:42,964 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:36:42,964 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-02-26T20:36:42,964 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:36:50,966 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:36:50,966 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:36:51,590 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:36:51,590 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]33019
2022-02-26T20:36:51,590 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:36:51,590 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:36:51,590 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:36:51,591 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:36:51,591 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:36:51,591 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:36:51,592 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907811592
2022-02-26T20:36:51,592 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:36:51,592 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907811592
2022-02-26T20:36:51,612 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:36:51,681 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:36:51,681 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:36:51,681 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:36:51,681 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:36:51,681 [INFO ] KQueueEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:36:51,682 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:36:51,681 [INFO ] KQueueEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:36:51,682 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:36:51,682 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:36:51,682 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:36:51,682 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:36:51,682 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:36:51,682 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:36:51,682 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:36:51,682 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:36:51,682 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:36:51,682 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:36:51,682 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:36:51,682 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:36:51,682 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:36:51,682 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:36:51,682 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:36:51,682 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:36:51,682 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:36:51,683 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/private/var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/models/d8e5c78a5f2a40899ccf2bc1b24393d3/mnist_handler.py", line 2, in <module>
2022-02-26T20:36:51,683 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:36:51,683 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:36:51,683 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-02-26T20:36:51,683 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:36:51,683 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:36:51,683 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:36:51,683 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-02-26T20:36:51,683 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-02-26T20:36:51,683 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-02-26T20:36:51,683 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:36:51,683 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:36:51,683 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:37:04,683 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:37:04,683 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:37:05,240 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:37:05,240 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]33023
2022-02-26T20:37:05,240 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:37:05,240 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:37:05,240 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:37:05,240 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:37:05,240 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:37:05,240 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:37:05,242 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907825242
2022-02-26T20:37:05,242 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907825242
2022-02-26T20:37:05,242 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:37:05,276 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:37:05,370 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:37:05,370 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:37:05,371 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:37:05,371 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:37:05,371 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:37:05,371 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:37:05,371 [INFO ] KQueueEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:37:05,371 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:37:05,371 [INFO ] KQueueEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:37:05,371 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:37:05,371 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:37:05,371 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:37:05,371 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:37:05,372 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:37:05,371 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:37:05,372 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:37:05,371 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:37:05,372 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:37:05,372 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:37:05,372 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:37:05,372 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:37:05,372 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:37:05,372 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:37:05,372 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:37:05,372 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/private/var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/models/d8e5c78a5f2a40899ccf2bc1b24393d3/mnist_handler.py", line 2, in <module>
2022-02-26T20:37:05,373 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-02-26T20:37:05,373 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-02-26T20:37:05,373 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-02-26T20:37:05,373 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-02-26T20:37:05,373 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-02-26T20:37:05,374 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-02-26T20:37:05,374 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2022-02-26T20:37:05,374 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-02-26T20:37:05,374 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2022-02-26T20:37:05,374 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:37:05,374 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 189, in <module>
2022-02-26T20:37:05,374 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-26T20:37:05,374 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 161, in run_server
2022-02-26T20:37:05,374 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:37:05,374 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:37:05,374 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-26T20:37:05,374 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:37:05,374 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:37:05,375 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-02-26T20:37:05,375 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-02-26T20:37:05,375 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:37:05,374 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py", line 123, in handle_connection
2022-02-26T20:37:05,375 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:37:05,375 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:37:05,375 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:37:23,772 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907843
2022-02-26T20:37:23,773 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:255.15348434448242|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907843
2022-02-26T20:37:23,773 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907843
2022-02-26T20:37:23,773 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907843
2022-02-26T20:37:23,773 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5451.8359375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907843
2022-02-26T20:37:23,773 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8234.71875|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907843
2022-02-26T20:37:23,773 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:66.7|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907843
2022-02-26T20:37:26,376 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:37:26,376 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:37:26,976 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:37:26,977 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]33028
2022-02-26T20:37:26,977 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:37:26,977 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:37:26,977 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:37:26,977 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:37:26,977 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:37:26,977 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:37:26,979 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907846979
2022-02-26T20:37:26,979 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907846979
2022-02-26T20:37:26,979 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:37:27,005 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:37:27,080 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:37:27,081 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:37:27,081 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:37:27,081 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:37:27,081 [INFO ] KQueueEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:37:27,081 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:37:27,081 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:37:27,081 [INFO ] KQueueEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:37:27,081 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:37:27,081 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:37:27,081 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:37:27,081 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:37:27,081 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:37:27,081 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:37:27,081 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:37:27,081 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:37:27,081 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:37:27,082 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:37:27,082 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:37:27,082 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:37:27,082 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:37:27,082 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:37:27,082 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:37:27,082 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:37:27,082 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/private/var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/models/d8e5c78a5f2a40899ccf2bc1b24393d3/mnist_handler.py", line 2, in <module>
2022-02-26T20:37:27,082 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-02-26T20:37:27,082 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-02-26T20:37:27,082 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:37:27,082 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:37:27,082 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-02-26T20:37:27,082 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:37:27,082 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:37:27,082 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:37:27,082 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2022-02-26T20:37:27,082 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:37:27,083 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:37:27,083 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-02-26T20:37:27,083 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:37:27,083 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-02-26T20:38:01,085 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:38:01,085 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:38:01,835 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:38:01,836 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]33039
2022-02-26T20:38:01,836 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:38:01,836 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:38:01,836 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:38:01,836 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-26T20:38:01,837 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:38:01,837 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:38:01,838 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907881838
2022-02-26T20:38:01,838 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907881838
2022-02-26T20:38:01,838 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:38:01,864 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:38:01,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-26T20:38:01,998 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:38:01,998 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 83, in load
2022-02-26T20:38:01,998 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-02-26T20:38:01,998 [INFO ] KQueueEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:38:01,998 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_loader.py", line 123, in _load_handler_file
2022-02-26T20:38:01,998 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-02-26T20:38:01,998 [INFO ] KQueueEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-26T20:38:01,998 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-02-26T20:38:01,998 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:38:01,998 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-02-26T20:38:01,998 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-26T20:38:01,998 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-02-26T20:38:01,998 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-02-26T20:38:01,998 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:38:01,998 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-02-26T20:38:01,998 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-02-26T20:38:01,999 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-02-26T20:38:01,999 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:38:01,999 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2022-02-26T20:38:01,999 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-02-26T20:38:01,999 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:38:01,999 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-26T20:38:01,999 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-02-26T20:38:01,999 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/private/var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/models/d8e5c78a5f2a40899ccf2bc1b24393d3/mnist_handler.py", line 2, in <module>
2022-02-26T20:38:01,999 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:38:01,999 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2022-02-26T20:38:01,999 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2022-02-26T20:38:01,999 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:38:01,999 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2022-02-26T20:38:01,999 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:38:01,999 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/torch_handler/image_classifier.py", line 8, in <module>
2022-02-26T20:38:01,999 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2022-02-26T20:38:01,999 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:38:01,999 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2022-02-26T20:38:01,999 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2022-02-26T20:38:01,999 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2022-02-26T20:39:08,226 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-26T20:39:08,226 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-26T20:39:08,820 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages
Current directory: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist
Temp directory: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4096 M
Python executable: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python
Config file: ./model-server/config.properties
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Initial Models: mnist=mnist.mar
Log dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Metrics dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Model config: N/A
2022-02-26T20:39:08,820 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages
Current directory: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist
Temp directory: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T/
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4096 M
Python executable: /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python
Config file: ./model-server/config.properties
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Initial Models: mnist=mnist.mar
Log dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Metrics dir: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/sasirekhakambhampaty/Documents/competitive/OxfordHack/serve/examples/image_classifier/mnist/model-server/model-store
Model config: N/A
2022-02-26T20:39:08,829 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-26T20:39:08,829 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-26T20:39:08,850 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnist.mar
2022-02-26T20:39:08,850 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnist.mar
2022-02-26T20:39:09,044 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnist
2022-02-26T20:39:09,044 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnist
2022-02-26T20:39:09,044 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnist
2022-02-26T20:39:09,044 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnist
2022-02-26T20:39:09,044 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnist loaded.
2022-02-26T20:39:09,044 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnist loaded.
2022-02-26T20:39:09,044 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnist, count: 1
2022-02-26T20:39:09,044 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnist, count: 1
2022-02-26T20:39:09,057 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:39:09,057 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/bin/python, /Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000]
2022-02-26T20:39:09,059 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2022-02-26T20:39:09,059 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2022-02-26T20:39:09,137 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-26T20:39:09,137 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-26T20:39:09,138 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2022-02-26T20:39:09,138 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2022-02-26T20:39:09,139 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-26T20:39:09,139 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-26T20:39:09,139 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2022-02-26T20:39:09,139 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2022-02-26T20:39:09,140 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-26T20:39:09,140 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-26T20:39:09,429 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:39:09,429 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-26T20:39:09,513 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907949
2022-02-26T20:39:09,514 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:255.10364532470703|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907949
2022-02-26T20:39:09,514 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907949
2022-02-26T20:39:09,515 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907949
2022-02-26T20:39:09,515 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5483.40625|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907949
2022-02-26T20:39:09,515 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8177.93359375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907949
2022-02-26T20:39:09,516 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:66.5|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907949
2022-02-26T20:39:09,768 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:39:09,769 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]33128
2022-02-26T20:39:09,769 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-26T20:39:09,769 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-02-26T20:39:09,769 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change null -> WORKER_STARTED
2022-02-26T20:39:09,769 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change null -> WORKER_STARTED
2022-02-26T20:39:09,774 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:39:09,774 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000
2022-02-26T20:39:09,783 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /var/folders/2r/3bt6dykx5yb757sz670pp2hh0000gn/T//.ts.sock.9000.
2022-02-26T20:39:09,785 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907949785
2022-02-26T20:39:09,785 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907949785
2022-02-26T20:39:09,813 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2022-02-26T20:39:11,464 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-02-26T20:39:11,470 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1657
2022-02-26T20:39:11,470 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1657
2022-02-26T20:39:11,471 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-02-26T20:39:11,471 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-02-26T20:39:11,471 [INFO ] W-9000-mnist_1.0 TS_METRICS - W-9000-mnist_1.0.ms:2418|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645907951
2022-02-26T20:39:11,471 [INFO ] W-9000-mnist_1.0 TS_METRICS - WorkerThreadTime.ms:29|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:null
2022-02-26T20:39:12,701 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907952701
2022-02-26T20:39:12,701 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645907952701
2022-02-26T20:39:12,702 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend received inference at: 1645907952
2022-02-26T20:39:12,703 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-02-26T20:39:12,703 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-26T20:39:12,703 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-02-26T20:39:12,703 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-02-26T20:39:12,703 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/service.py", line 102, in predict
2022-02-26T20:39:12,704 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-02-26T20:39:12,704 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/torch_handler/base_handler.py", line 216, in handle
2022-02-26T20:39:12,704 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-02-26T20:39:12,704 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/Users/sasirekhakambhampaty/opt/anaconda3/envs/python38/lib/python3.8/site-packages/ts/torch_handler/vision_handler.py", line 52, in preprocess
2022-02-26T20:39:12,704 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     image = torch.FloatTensor(image)
2022-02-26T20:39:12,705 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - TypeError: new(): data must be a sequence (got dict)
2022-02-26T20:39:12,706 [INFO ] W-9000-mnist_1.0 ACCESS_LOG - /127.0.0.1:55177 "POST /predictions/mnist HTTP/1.1" 503 11
2022-02-26T20:39:12,706 [INFO ] W-9000-mnist_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:null
2022-02-26T20:39:12,707 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.job.Job - Waiting time ns: 156193, Inference time ns: 6029459
2022-02-26T20:39:12,707 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.job.Job - Waiting time ns: 156193, Inference time ns: 6029459
2022-02-26T20:39:12,707 [INFO ] W-9000-mnist_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:null
2022-02-26T20:40:09,493 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908009
2022-02-26T20:40:09,493 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:255.10318756103516|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908009
2022-02-26T20:40:09,493 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908009
2022-02-26T20:40:09,494 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908009
2022-02-26T20:40:09,494 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5471.5859375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908009
2022-02-26T20:40:09,494 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8191.23828125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908009
2022-02-26T20:40:09,494 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:66.6|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908009
2022-02-26T20:41:09,480 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908069
2022-02-26T20:41:09,480 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:255.1015739440918|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908069
2022-02-26T20:41:09,480 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908069
2022-02-26T20:41:09,481 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908069
2022-02-26T20:41:09,481 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5318.12890625|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908069
2022-02-26T20:41:09,481 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8348.13671875|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908069
2022-02-26T20:41:09,481 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:67.5|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908069
2022-02-26T20:42:09,499 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908129
2022-02-26T20:42:09,499 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:255.09974670410156|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908129
2022-02-26T20:42:09,499 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908129
2022-02-26T20:42:09,500 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908129
2022-02-26T20:42:09,500 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5444.21484375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908129
2022-02-26T20:42:09,500 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8271.48046875|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908129
2022-02-26T20:42:09,500 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:66.8|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908129
2022-02-26T20:43:08,775 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645908188775
2022-02-26T20:43:08,775 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1645908188775
2022-02-26T20:43:08,778 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend received inference at: 1645908188
2022-02-26T20:43:08,806 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Calculating Explanations
2022-02-26T20:43:08,807 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Getting data and target
2022-02-26T20:43:08,875 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 98
2022-02-26T20:43:08,875 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 98
2022-02-26T20:43:08,875 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - input shape torch.Size([1, 1, 28, 28])
2022-02-26T20:43:08,875 [INFO ] W-9000-mnist_1.0 ACCESS_LOG - /127.0.0.1:55203 "PUT /explanations/mnist HTTP/1.1" 200 101
2022-02-26T20:43:08,875 [INFO ] W-9000-mnist_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:null
2022-02-26T20:43:08,875 [INFO ] W-9000-mnist_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:93.38|#ModelName:mnist,Level:Model|#hostname:Sasirekhas-MacBook-Pro.local,requestID:0de70253-7a56-40c7-8c5b-06bc356f640c,timestamp:1645908188
2022-02-26T20:43:08,876 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.job.Job - Waiting time ns: 95930, Backend time ns: 100561110
2022-02-26T20:43:08,876 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.job.Job - Waiting time ns: 95930, Backend time ns: 100561110
2022-02-26T20:43:08,876 [INFO ] W-9000-mnist_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:93.44|#ModelName:mnist,Level:Model|#hostname:Sasirekhas-MacBook-Pro.local,requestID:0de70253-7a56-40c7-8c5b-06bc356f640c,timestamp:1645908188
2022-02-26T20:43:08,876 [INFO ] W-9000-mnist_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:null
2022-02-26T20:43:08,876 [INFO ] W-9000-mnist_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:null
2022-02-26T20:43:09,480 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908189
2022-02-26T20:43:09,480 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:255.10298538208008|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908189
2022-02-26T20:43:09,480 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908189
2022-02-26T20:43:09,481 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908189
2022-02-26T20:43:09,481 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5398.01171875|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908189
2022-02-26T20:43:09,481 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8321.9375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908189
2022-02-26T20:43:09,481 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:67.1|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908189
2022-02-26T20:44:09,476 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908249
2022-02-26T20:44:09,477 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:255.11044692993164|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908249
2022-02-26T20:44:09,477 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908249
2022-02-26T20:44:09,478 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908249
2022-02-26T20:44:09,478 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:5448.50390625|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908249
2022-02-26T20:44:09,478 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:8275.7734375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908249
2022-02-26T20:44:09,478 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:66.7|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908249
2022-02-26T20:45:09,516 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908309
2022-02-26T20:45:09,517 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:255.10662460327148|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908309
2022-02-26T20:45:09,517 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908309
2022-02-26T20:45:09,517 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908309
2022-02-26T20:45:09,518 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:5091.99609375|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908309
2022-02-26T20:45:09,518 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:8606.1328125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908309
2022-02-26T20:45:09,518 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:68.9|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908309
2022-02-26T20:46:09,519 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908369
2022-02-26T20:46:09,520 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:255.1049041748047|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908369
2022-02-26T20:46:09,520 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908369
2022-02-26T20:46:09,520 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908369
2022-02-26T20:46:09,520 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:5358.78125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908369
2022-02-26T20:46:09,520 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:8362.7578125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908369
2022-02-26T20:46:09,520 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:67.3|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908369
2022-02-26T20:47:09,472 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908429
2022-02-26T20:47:09,472 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:255.10417556762695|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908429
2022-02-26T20:47:09,472 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908429
2022-02-26T20:47:09,472 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908429
2022-02-26T20:47:09,473 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:5485.76953125|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908429
2022-02-26T20:47:09,473 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:8235.7890625|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908429
2022-02-26T20:47:09,474 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:66.5|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908429
2022-02-26T20:48:09,515 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908489
2022-02-26T20:48:09,516 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:255.1086082458496|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908489
2022-02-26T20:48:09,516 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:14.018115997314453|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908489
2022-02-26T20:48:09,516 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:5.2|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908489
2022-02-26T20:48:09,517 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5422.1796875|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908489
2022-02-26T20:48:09,517 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8311.4296875|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908489
2022-02-26T20:48:09,517 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:66.9|#Level:Host|#hostname:Sasirekhas-MacBook-Pro.local,timestamp:1645908489
